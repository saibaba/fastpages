{
  
    
        "post0": {
            "title": "Title",
            "content": "How to solve for a linear system of equations iteratively? . First let&#39;s consider a single variable case . Given $3x = 6$, find $x$. . Split $3$ as $M+B$ . $(M+B)x = 6$ . $Mx = -Bx + 6$ . This splitting allows us to start with a random $x_0$, compute $-Bx_0+6$ which is $Mx_1$, extract $x_1$ from it and hopefully, the extracted $x_1 = frac{-Bx_0 + 6}{M}$ is better than initial random selection. Now we can continue this, for example we can perform the same step with the new value of again: . $$Mx_2 = -Bx_1+6 = -B( frac{(-Bx_0+6)}{M} ) + 6$$ . So, the pattern seems to be: . x = -B (-Bx/M +6/M) + 6 = B^2 x/M - 6B/M + 6 . Let&#39;s seprately group the coefficients: . Start with $x = x_0$ . $Mx_1 = -Bx_0 + 6$ . or . $x_1 = ( frac{-B}{M}) x_0 + ( frac{6}{M})$ . Let . -B/M = P 6/M = z . Continuing, . $ begin{aligned} x_{i+1} &amp;= P * x_{i} + z &amp;= P * [ P * x_{i-1} + z ] + z &amp;= P^2 x_{i-1} + P z + z end{aligned}$ . Error analysis: . Let the error in any step be distance from true value (which is not known): . $e_0 = x_0 - x$ . $e_1 = x_1 - x$ . ... . $e_i = x_i - x$ . Or . $x_i = e_i + x$ . And so on. Note that this is analytical purpose only and error is not used to drive the algorithm in calculating approximation (error depends on the true value which we do not know - so can&#39;t use it to drive/stop algorithm)!. . $ begin{aligned} x_{i+1} &amp;= P * x_{i} + z &amp;= P (x + e_{i}) + z &amp;= P x + z + P e_{i} &amp;= x + P e_{i} end{aligned}$ . This is because Px + z = x. . So, . $ begin{aligned} e_{i+1} = x_{i+1} - x = P e_i + x - x = P e_{i} end{aligned}$ . So, error starts decreasing if P &lt; 1 and positive. Even if P &lt; 1, if it is negative, convergence is questionable. . If we choose values as below, it satisfies the need: . B = -1 M = 4 (verify that B+M = 3) P = 1/4 z = 6/4 . Let&#39;s continue this for few iterations and see what we get. . B = -1 M = 4 P = 0.25 z = 1.5 x0 = 1.1 x = 2 e0 = x0-x e = [e0] ec = e0 xl = [x0] xc = x0 for i in range(10): en = P * ec e.append(en) ec = en xl.append(xc) xn = P * xc + z xl.append(xn) xc = xn print(e) print(xl) # Note that x is used only for error analysis not for actual calculation of x . [-0.8999999999999999, -0.22499999999999998, -0.056249999999999994, -0.014062499999999999, -0.0035156249999999997, -0.0008789062499999999, -0.00021972656249999998, -5.4931640624999995e-05, -1.3732910156249999e-05, -3.4332275390624997e-06, -8.583068847656249e-07] [1.1, 1.1, 1.775, 1.775, 1.94375, 1.94375, 1.9859375, 1.9859375, 1.996484375, 1.996484375, 1.99912109375, 1.99912109375, 1.9997802734375, 1.9997802734375, 1.999945068359375, 1.999945068359375, 1.9999862670898438, 1.9999862670898438, 1.9999965667724608, 1.9999965667724608, 1.9999991416931153] . Extrapolate above to multiple variable (aha! matrix) case . 2x+3y = 7 . 3x+2y = 8 . $ begin{aligned} begin{bmatrix} 2 &amp;&amp; 3 3 &amp;&amp; 2 end{bmatrix} . begin{bmatrix} x y end{bmatrix} &amp;= begin{bmatrix} 7 8 end{bmatrix} end{aligned}$ | . A x = b . A is square and in general may be or may not be invertible. . Split A = M + B . (M+B)x = b . Mx = -Bx + b . Let&#39;s use an M such that it is invertible (and easily invertible) . $x = M^{-1}(-Bx+b)$ . Start with random $x = x_0 = begin{bmatrix}0 &amp;&amp; 0 end{bmatrix}^T$ . $ begin{aligned} x_1 &amp;= M^{-1}(-Bx_0+b) &amp;= -M^{-1} B x_0 + M^{-1} b end{aligned}$ . $ begin{aligned} x_2 &amp;= M^{-1} (-Bx_1+b) &amp;= -M^{-1} B x_1 + M^{-1} b &amp;= -M.T B ( -M.T B x0 + M^{-1} b ) + M^{-1} b &amp;= M^{-1} B M^{-1} B x0 - M^{-1} B M^{-1} b + M^{-1} b end{aligned}$ . The -ve term is concerning, it would be tough to analyze convergence. . Let&#39;s try something different, may be we could do subtraction. Let&#39;s split like this: . A = M - N . Mx - Nx = b . $x = M^{-1} N x + M^{-1} b$ . $x_1 = M^{-1} N x_0 + M^{-1} b$ . Let $M^{-1} N = P$ and $M^{-1} b = z$ for ease. . x_1 = P x_0 + z . $ begin{aligned} x_2 = P (x_1) + z = P (P x_0 + z) + z = P^2 x_0 + P z + z end{aligned}$ . Error analysis . $e_0 = x_0-x$ . or . $e_i = x_i - x$ . $ begin{aligned} x_{i+1} &amp;= P x_i + z &amp;= P (e_i + x) + z &amp;= P e_i + P x + z &amp;= P e_i + x end{aligned}$ . $ begin{aligned} e_{i+1} &amp;= x_{i+1} - x &amp;= P e_i &amp;= P^{i+1} e_0 end{aligned}$ . So, we want $P^k -&gt; 0$ in the limit for error to converge to zero. . But, $P^k$ -&gt; 0 iff $||P^k||$ -&gt; 0, iff $ rho(P)$ &lt; 1. . Now, how do we find M and N? . Since we want M to be easily invertible, it would be so if it is a diagonal matrix. May be we could just take diagonal elements of A and shove in M. Remaining ones go into -N. Then A = M - N. . Then $P = M^{-1} N = M^{-1} (M-A) = I - M^{-1} A $ . or $M^{-1} A = I - P $. Since $M^{-1} A$ is invertible, $I - P$ is also invertible. In this case, If $ rho(P) le 1$, iterative method is convergent. A simpler way is to verify convergence is that M is diagonally dominant (see details in this reference: https://math.stackexchange.com/questions/2812573/proving-the-jacobi-method-converges-for-diagonally-column-dominant-matrices/2813489). Also see page 68 of http://www.robots.ox.ac.uk/~sjrob/Teaching/EngComp/linAlg34.pdf. . More on this: . $ ||e_{i+1}|| = || P e_i || le ||P||_{p} ||e_i||$ for any norm p. If $||P||_{p} lt 1$, the error keeps decreasing. . If A is diagonally dominated, then the ‘iteration matrix’ P has an ∞-norm that is strictly less than 1. . Note that for some norms, p, $||P||_{p}$ might be less than 1 and may be greater than 1 for some other norms. So, this is only sufficient condition. The necessary condition is that $ rho(P) le 1$. . Let&#39;s try on an example (not on the one introduced in this section, though). . import numpy as np from numpy import linalg as LA # works only for diagonally dominant A (equivalent to convergence of P) # x is used for error analysis only def solve(A, b, x, num_iter = 60): variable_count = len(x) x0 = np.zeros((variable_count, 1)) e0 = x0-x e = [e0] ec = e0 xl = [x0] xc = x0 M = np.diag(np.diag(A)) N = M - A P = LA.inv(M).dot(N) z = LA.inv(M).dot(b) eig = LA.eig(P) print(&quot;Eigen stuff of P&quot;) print(eig) print(&quot;Initial&quot;) print(&quot;M inverse: &quot;) print(LA.inv(M)) print(&quot;P (inv(M)*N): &quot;) print(P) print (&quot;z (inv(M) b)&quot;) print(z) #print(&quot;looping&quot;) for i in range(num_iter): en = P.dot(ec) e.append(en) ec = en #xl.append(xc) xn = P.dot(xc) + z xl.append(xn) xc = xn #print(&quot;Iter #&quot; + str(i)) #print(&quot;Error after iteration&quot;) #print(en) #print(&quot;Result after iteration&quot;) #print(xn) #print(&quot;-&quot;) return (e, xl) . import numpy as np from numpy import linalg as LA A = np.array([[2, -1, 0, 0], [-1, 2, -1, 0], [0, -1, 2, -1], [0, 0, -1, 2]]) b = np.array([[25], [-24], [21], [-15]]) x = np.array([[11], [-3], [7], [-4]]) el, xl = solve(A, b, x, 60) print(&quot;Result after last iteration&quot;) print(xl[-1]) . Eigen stuff of P (array([-0.80901699, -0.30901699, 0.80901699, 0.30901699]), array([[ 0.37174803, 0.60150096, -0.37174803, -0.60150096], [-0.60150096, -0.37174803, -0.60150096, -0.37174803], [ 0.60150096, -0.37174803, -0.60150096, 0.37174803], [-0.37174803, 0.60150096, -0.37174803, 0.60150096]])) Initial M inverse: [[0.5 0. 0. 0. ] [0. 0.5 0. 0. ] [0. 0. 0.5 0. ] [0. 0. 0. 0.5]] P (inv(M)*N): [[0. 0.5 0. 0. ] [0.5 0. 0.5 0. ] [0. 0.5 0. 0.5] [0. 0. 0.5 0. ]] z (inv(M) b) [[ 12.5] [-12. ] [ 10.5] [ -7.5]] Result after last iteration [[10.99998147] [-2.99998811] [ 6.99997002] [-3.99999265]] . import numpy as np from numpy import linalg as LA # Above method does not work for this as A is not diagonally dominant matrix A = np.array([[2, 3], [3, 2]]) b = np.array([[7], [8]]) x = np.array([[2], [1]]) el, xl = solve(A, b, x) print(&quot;Result after last iteration&quot;) print(xl[-1]) . Eigen stuff of P (array([ 1.5, -1.5]), array([[ 0.70710678, 0.70710678], [-0.70710678, 0.70710678]])) Initial M inverse: [[0.5 0. ] [0. 0.5]] P (inv(M)*N): [[ 0. -1.5] [-1.5 0. ]] z (inv(M) b) [[3.5] [4. ]] Result after last iteration [[-7.35369374e+10] [-3.67684687e+10]] . A = np.array([[4, 2, 3], [3, -5, 2], [-2, 3, 8]]) b = np.array([[8], [-14], [27]]) x = np.array([[-1], [3], [2]]) e, xl = solve(A, b, x) print(&quot;Result after last iteration&quot;) print(xl[-1]) . Eigen stuff of P (array([-0.08875095+0.81309913j, -0.08875095-0.81309913j, 0.1775019 +0.j ]), array([[ 0.66666828+0.j , 0.66666828-0.j , -0.55176491+0.j ], [-0.22848143-0.58075007j, -0.22848143+0.58075007j, -0.6277451 +0.j ], [ 0.23121087-0.33558982j, 0.23121087+0.33558982j, 0.54908249+0.j ]])) Initial M inverse: [[ 0.25 0. 0. ] [-0. -0.2 -0. ] [ 0. 0. 0.125]] P (inv(M)*N): [[ 0. -0.5 -0.75 ] [ 0.6 0. 0.4 ] [ 0.25 -0.375 0. ]] z (inv(M) b) [[2. ] [2.8 ] [3.375]] Result after last iteration [[-0.99999138] [ 2.99997978] [ 1.99999301]] . Convergence Analysis of error continued . What if $e_0$ = an eigenvector of P, say $v_0$ with eigenvalue $ lambda_0$? . $ begin{aligned} e_{i+1} &amp;= P^k v_0 &amp;= P^{k-1} P v_0 &amp;= P^{k-1} lambda_0 v_0 &amp;= lambda_0 P^{k-1} v_0 &amp;= lambda^{k}_0 v_0 end{aligned}$ . If $ 0 le lambda_0 lt 1$, error goes to zero. . What if $e_0$ is not an eigenvector? . It can be written as a linear combination of eigenbasis (orthonormal eigenvectors of P), say Q. . Let $v_1, v_2, dots, v_n$ be the orthonormal eigenvectors of P. Let $ lambda_1, lambda_2, dots, lambda_n$ be corresponding eigenvalues. . $Q = [v_1 dots v_n]$ . $e_0 = c_1 v_1 + c_2 v_2 + dots + c_n v_n$ . $e_{i+1} = P^{i+1} e_0 = P^{i+1} (c_1 v_1 + c_2 v_2 + dots + c_n v_n) = c_1 lambda_1^{i+1} v_1 + c_2 lambda_2^{i+1} v_2 + dots + c_n lambda_n^{i+1} v_n$. . Iff $| lambda_i| lt 1$ for all i, $e_k -&gt; 0$. . Also, . Spectral radius, $ rho = max_i(| lambda_i|)$. . $||e_k|| -&gt; ||c_j lambda_j^k v_j||$ ~ $ rho^k$ for some j and reaches zero if $ rho lt 1$. . Using matrices: . P = $QDQ^{-1}$ where D is the diagonal matrix of eigenvalues. . $e_{i+1} = (QDQ^{-1})^{i+1}e_0 = QD^{i+1}Q^{-1}e_0$. So, if eigenvalues &lt; 1, error goes to zero. . If P does not have full matrix, you can use Jordan form of P as matrix similarity extends to Jordan form as well (see this reference for example: https://math.la.asu.edu/~gardner/iter.pdf). . Convergence Analysis of solution . How about convergence of $x_{i+1}$? . Let $x^*$ be the (unknown) solution. Then it satisfies $x^* = Px^* + z$ by definition. . Consider, . $x_{i+1} - x^* = (Px_i+z)-(Px^*+z) = P(x_i-x^*) = P((Px_{i-1}+z) - (Px^* + z)) = P^2(x_{i-1}-x^*) = dots = P^{i+1}(x_0-x^*)$. . So, if the $ rho(P) lt 1$, then $||x_{i+1} - x^*|| = ||P^{i+1}(x_0-x^*)||$ ~ $0$ or $||x_{i+1}||$ ~ $||x^*||$. . from numpy.linalg import matrix_power A = np.array([[4, 2, 3], [3, -5, 2], [-2, 3, 8]]) b = np.array([[8], [-14], [27]]) x_solution = np.array([[-1], [3], [2]]) x_0 = np.zeros((3, 1)) M = np.diag(np.diag(A)) N = M - A P = LA.inv(M).dot(N) # consider 11th iteration i = 11 # grab x_11 from the output above x_11 = np.array([[-0.66699081], [ 2.85785307], [ 2.09930262]]) lhs = x_11 - x_solution rhs = (matrix_power(P, i+1)).dot(x_0-x_solution) print(lhs) print(rhs) . [[ 0.33300919] [-0.14214693] [ 0.09930262]] [[ 0.33300919] [-0.14214693] [ 0.09930262]] . Now you could see that, the rate at which each component of $x_{i+1}$ converges depends on the corresponding eigenvecgtor of $P$. For, . $x_{i+1} - x^* = QD^{i+1}Q^{-1} (x_{0} - {x^*})$ . Let&#39;s simplify as we are using $x_0 = 0$ most of the time. . $x_{i+1} = x^* - QD^{i+1}Q^{-1} {x^*}$ . Let&#39;s try to study how individual component of $x_{i+1}$ progresses towards the corresponding component in the solution as we iterate. . Let $q_{(j)}$ be j-th row of Q and $q&#39;_{(j)}$ be j-th row of $Q^{-1}$. Then after mundane manipulations of above equation, we see that . ${x_{i+1}}_{(j)} = q_{(j)} sum^{n}_{k=1} { lambda_k}^{i+1} q&#39;_k x^*$ . This is way too complex to see what is going on - there is a light at the end of tunnel, though - we just need to change our frame of reference to that of eigenbasis of P. . Let . $Q^{-1}v = w$ . Rewrite above using w. First multiply both sides with $Q^{-1}$: . $Q^{-1}x_{i+1} = Q^{-1}x^* - D^{i+1}Q^{-1} {x^*}$ . So, . $w_{i+1} = w^* - D^{i+1} w^*$ . Now, look at the j-th component of $w_{i+1}$: . ${w_{i+1}}_{(j)} = {w^*}_{(j)} - { lambda_j}^{i+1} {w^*}_{(j)} = (1-{ lambda_j}^{i+1}) {w^*}_{(j)}$. . Now, we clearly see that each component of $w$ takes its own progression towards meeting $w^*$ as long as $ lambda_j lt 1$. Also, note that it is in closed form, you do not need to loop to find value at any i unlike for $x_{i+1}$ which you cannot compute without looping. . Let&#39;s test above theory. First, a couple of utilities: . def analyze(A, b, x, num_iter = 60): variable_count = len(x) x0 = np.zeros((variable_count, 1)) e0 = x0-x e = [e0] ec = e0 xl = [x0] xc = x0 M = np.diag(np.diag(A)) N = M - A P = LA.inv(M).dot(N) z = LA.inv(M).dot(b) eigvals, eigvecs = LA.eig(P) D = np.diag(eigvals) Q = np.array(eigvecs) Qinv = LA.inv(Q) wstar = Qinv.dot(x) d0 = Qinv.dot(e0) dc = d0 dl = [d0] print(&quot;wstar&quot;) print(wstar) print(&quot;--&quot;) wl = [] Dc = D for i in range(num_iter): en = P.dot(ec) e.append(en) ec = en dn = Qinv.dot(en) dl.append(dn) dc = dn xn = P.dot(xc) + z xl.append(xn) wl.append(wstar - Dc.dot(wstar)) Dc = Dc * D xc = xn return (e, dl, xl, np.array(wl)) . import matplotlib.pyplot as plt def plot_points(points): plt.scatter(np.real(points), np.imag(points)) plt.show() x = np.real(points) y = np.imag(points) n = [i for i in range(len(x))] fig, ax = plt.subplots(figsize=(20,10)) ax.scatter(x, y) for i, txt in enumerate(n): ax.annotate(txt, (x[i], y[i])) plt.xticks(np.arange(min(x)-0.2,max(x)+0.2, 0.1)) plt.yticks(np.arange(min(y)-0.2,max(y)+0.2, 0.1)) plt.show() def print_error_contributions(dl, eigvals): c = len(dl[0]) n = len(dl) x = [i for i in range(n)] y_prev_min = [0 for k in range(n)] y_prev_max = [0 for k in range(n)] colors = [&#39;red&#39;, &#39;blue&#39;, &#39;black&#39;] fig, vax = plt.subplots(1, 1, figsize=(24, 12)) for comp in range(c): y_min = y_prev_max y_max = [np.absolute(dl[k][comp])[0]+y_min[k] for k in range(n)] vax.vlines(x, y_min, y_max, color=colors[comp], label=np.absolute(eigvals[comp])) y_prev_min = y_min y_prev_max = y_max plt.legend() plt.show() . tol = 1e-13 A = np.array([[4, 2, 3], [3, -5, 2], [-2, 3, 8]]) b = np.array([[8], [-14], [27]]) x_solution = np.array([[-1], [3], [2]]) x_0 = np.zeros((3, 1)) M = np.diag(np.diag(A)) N = M - A P = LA.inv(M).dot(N) w, v = LA.eig(P) D = np.diag(w) print(&quot;Diagonal&quot;) print(D) print(&quot;&quot;) Q = np.array(v) print(&quot;Eigenbasis&quot;) print(Q) print(&quot;&quot;) Qinv = LA.inv(Q) e, dl, xl, wl = analyze(A, b, x_solution) wl.real[abs(wl.real) &lt; tol] = 0.0 wl.imag[abs(wl.imag) &lt; tol] = 0.0 w_lastiter = wl[-1] x_lastiter = Q.dot(w_lastiter) print(&quot;value of x after last iteration&quot;) print(x_lastiter) print(xl[-1]) . Diagonal [[-0.08875095+0.81309913j 0. +0.j 0. +0.j ] [ 0. +0.j -0.08875095-0.81309913j 0. +0.j ] [ 0. +0.j 0. +0.j 0.1775019 +0.j ]] Eigenbasis [[ 0.66666828+0.j 0.66666828-0.j -0.55176491+0.j ] [-0.22848143-0.58075007j -0.22848143+0.58075007j -0.6277451 +0.j ] [ 0.23121087-0.33558982j 0.23121087+0.33558982j 0.54908249+0.j ]] wstar [[-0.47309829+2.75837488e+00j] [-0.47309829-2.75837488e+00j] [ 0.66912693+2.82446075e-16j]] -- value of x after last iteration [[-0.99999138+0.j] [ 2.99997978+0.j] [ 1.99999301+0.j]] [[-0.99999138] [ 2.99997978] [ 1.99999301]] . plot_points([item[0][0] for item in wl]) . wl_2 = [item[1][0] for item in wl] plot_points(wl_2) . wl_3 = [item[2][0] for item in wl] plot_points(wl_3) . print_error_contributions(dl, w) . print([np.absolute(item[0][0]) for item in dl]) . [2.798652169004236, 2.2890971880511395, 1.872317680052403, 1.5314218694319996, 1.2525934926325977, 1.0245318348284158, 0.837993720030255, 0.6854188917689926, 0.5606236013044015, 0.45854998470839503, 0.3750610712550078, 0.3067731149536745, 0.2509184537426817, 0.20523333812392872, 0.1678661830137287, 0.13730252432274404, 0.11230363880887148, 0.09185633950958826, 0.07513191199851137, 0.06145252718200072, 0.050263769370455534, 0.0411121662066704, 0.03362680975530614, 0.02750432386936806, 0.022496568571799172, 0.01840058311956207, 0.01505036014978504, 0.012310117519994572, 0.010068795154928024, 0.008235553861060674, 0.006736093679017377, 0.005509642559323132, 0.004506493314673282, 0.003685989023159045, 0.0030148752322811316, 0.002465952179757755, 0.00201697240659974, 0.0016497390834985244, 0.0013493685063400318, 0.0011036868703147007, 0.0009027368743094845, 0.0007383741586104869, 0.0006039372198247381, 0.0004939774249088327, 0.0004040381819659562, 0.000330474317761553, 0.000270304341457427, 0.0002210896069190208, 0.00018083547613054456, 0.0001479104779418167, 0.00012098018570860147, 9.895313393446755e-05, 8.09365819543167e-05, 6.620033179329207e-05, 5.414713376474904e-05, 4.4288480367325094e-05, 3.6224807425058966e-05, 2.962930003691752e-05, 2.4234647002440263e-05, 1.982220688983876e-05, 1.6213146655035137e-05] . print([np.absolute(item[1][0]) for item in dl]) . [2.798652169004236, 2.2890971880511395, 1.8723176800524033, 1.5314218694319996, 1.252593492632598, 1.0245318348284158, 0.837993720030255, 0.6854188917689925, 0.5606236013044015, 0.45854998470839503, 0.3750610712550078, 0.3067731149536745, 0.2509184537426817, 0.20523333812392872, 0.16786618301372866, 0.13730252432274404, 0.11230363880887148, 0.09185633950958826, 0.07513191199851135, 0.06145252718200072, 0.05026376937045553, 0.04111216620667041, 0.03362680975530614, 0.02750432386936806, 0.022496568571799175, 0.01840058311956207, 0.015050360149785039, 0.012310117519994572, 0.010068795154928024, 0.008235553861060674, 0.006736093679017377, 0.005509642559323131, 0.004506493314673281, 0.0036859890231590448, 0.003014875232281132, 0.002465952179757755, 0.00201697240659974, 0.0016497390834985242, 0.0013493685063400316, 0.0011036868703147007, 0.0009027368743094846, 0.0007383741586104869, 0.0006039372198247381, 0.0004939774249088327, 0.0004040381819659562, 0.0003304743177615531, 0.00027030434145742697, 0.00022108960691902082, 0.0001808354761305445, 0.00014791047794181672, 0.00012098018570860146, 9.895313393446755e-05, 8.093658195431669e-05, 6.620033179329207e-05, 5.4147133764749035e-05, 4.42884803673251e-05, 3.6224807425058966e-05, 2.9629300036917512e-05, 2.4234647002440263e-05, 1.982220688983876e-05, 1.621314665503514e-05] . print([np.absolute(item[2][0]) for item in dl]) . [0.6691269288904338, 0.1187713010249305, 0.021082131562910722, 0.00374211840234584, 0.0006642331253550182, 0.00011790264160016495, 2.0927942864748528e-05, 3.714749615801871e-06, 6.593751137699833e-07, 1.1704033509030154e-07, 2.0774881828722336e-08, 3.687581129341311e-09, 6.545526343160191e-10, 1.1618425665993676e-10, 2.0622961671712406e-11, 3.6606759290804006e-12, 6.497302695921548e-13, 1.1530013092596007e-13, 2.049055399636148e-14, 3.6533318914677856e-15, 6.314481135683715e-16, 1.0283025326360291e-16, 3.134881669300665e-17, 7.969592015545964e-18, 8.935373837186346e-18, 6.9282887019832755e-19, 8.812661463699985e-18, 1.8673160853157835e-19, 4.4683740379945e-18, 2.6029252338254693e-18, 2.281162689856478e-18, 1.312097709798084e-18, 1.790569823012601e-18, 8.884593494774184e-19, 6.516035922184243e-19, 8.849165508992531e-19, 2.4398084866490184e-19, 8.790712237024587e-19, 2.027098600242056e-19, 4.470075600200349e-19, 1.121055986803255e-19, 2.8192285445507893e-19, 2.76201182912248e-20, 1.2060032019151485e-19, 5.452195462514497e-20, 1.3979849631489435e-19, 8.192580043583475e-20, 4.5841264822001056e-20, 5.520586729152878e-20, 4.573452570427438e-20, 4.167859354643055e-20, 1.7790799125307778e-20, 3.466035969235535e-20, 5.966257020288272e-21, 2.764623774768838e-20, 3.569907431211514e-21, 2.06885292569832e-20, 5.082687364439243e-21, 1.213638136781469e-20, 3.4129903461847316e-21, 6.1557787183658335e-21] . # A is diagonally dominant, so convergence is possible. # Since there are no imaginary numbers, no spiraling but linear movement of w&#39;s # https://www.maa.org/press/periodicals/loci/joma/iterative-methods-for-solving-iaxi-ibi-jacobis-method A = np.array([[4, -1, -1], [-2, 6, 1], [-1, 1, 7]]) b = np.array([[3], [9], [-6]]) x_solution = np.array([[1], [2], [-1]]) e, dl, xl, wl = analyze(A, b, x_solution, 50) M = np.diag(np.diag(A)) N = M - A P = LA.inv(M).dot(N) z = LA.inv(M).dot(b) w, v = LA.eig(P) D = np.diag(w) print(&quot;Eigenvalues in a diagonal matrix&quot;) print(D) print(&quot;&quot;) Q = np.array(v) print(&quot;Eigenbasis&quot;) print(Q) print(&quot;&quot;) Qinv = LA.inv(Q) print(&quot;value of x after last iteration:&quot;) print(xl[-1]) print(&quot;-&quot;) w_lastiter = wl[-1] x_from_w = Q.dot(w_lastiter) print(&quot;value of x after last iteration via w&quot;) print(x_from_w) . wstar [[ 0.09126133] [-1.62567469] [-1.14708505]] -- Eigenvalues in a diagonal matrix [[-0.42946179 0. 0. ] [ 0. 0.28202928 0. ] [ 0. 0. 0.14743251]] Eigenbasis [[ 0.62737263 -0.61970267 0.05639495] [-0.65211772 -0.78059393 -0.68915668] [-0.42561257 0.08149674 0.72241448]] value of x after last iteration: [[ 1.] [ 2.] [-1.]] - value of x after last iteration via w [[ 1.] [ 2.] [-1.]] . plot_points([item[0][0] for item in wl]) . plot_points([item[1][0] for item in wl]) . plot_points([item[2][0] for item in wl]) . print_error_contributions(dl, w) . print([np.absolute(item[0][0]) for item in dl]) . [0.09126132739786263, 0.03919325273556447, 0.01683200435269831, 0.007228702665756757, 0.003104451563520552, 0.0013332433156920064, 0.0005725770566771637, 0.00024589996587601074, 0.00010560463873409491, 4.5353156851522314e-05, 1.9477447781228038e-05, 8.364819527612026e-06, 3.592370341095475e-06, 1.5427857857522183e-06, 6.625675402924254e-07, 2.8454743976987896e-07, 1.2220225193020754e-07, 5.248119747234745e-08, 2.2538668843063196e-08, 9.67949699480334e-09, 4.1569740753009465e-09, 1.78526151431232e-09, 7.667015999501948e-10, 3.2926903910356256e-10, 1.4140846989131924e-10, 6.07295341567624e-11, 2.608101425418065e-11, 1.1200798984739615e-11, 4.810315146253706e-12, 2.0658465380731705e-12, 8.872021456208796e-13, 3.8101941876498335e-13, 1.6363328041143228e-13, 7.027424099537046e-14, 3.018010110815074e-14, 1.2961200149542785e-14, 5.566340176081721e-15, 2.390530398294518e-15, 1.0266414563963668e-15, 4.409032743292475e-16, 1.893511080261688e-16, 8.131906519697046e-17, 3.492343104533209e-17, 1.499827910003451e-17, 6.4411877421361995e-18, 2.766243997243003e-18, 1.1879960899486444e-18, 5.10198923572861e-19, 2.1911094137200286e-19, 9.40997764023488e-20, 4.0412258116944346e-20] . print([np.absolute(item[1][0]) for item in dl]) . [1.6256746930324613, 0.45848786611403614, 0.12930700359349495, 0.03646836135499692, 0.010285145761320065, 0.0029007122722585787, 0.0008180857988493498, 0.00023072415029908153, 6.507096640243248e-05, 1.8351917920416114e-05, 5.1757782307210415e-06, 1.459721017158865e-06, 4.116840700955752e-07, 1.1610696261696217e-07, 3.274556327867394e-08, 9.235207693573612e-09, 2.604598993079086e-09, 7.345731833913359e-10, 2.071711473403403e-10, 5.842832989377841e-11, 1.6478499916631844e-11, 4.647419496604932e-12, 1.31070838284425e-12, 3.6965814386087795e-13, 1.0425442082408956e-13, 2.9402799429343783e-14, 8.292450405925919e-15, 2.3387138323321306e-15, 6.595857824655651e-16, 1.860225045131712e-16, 5.2463793346307113e-17, 1.4796325957830212e-17, 4.1729971831730284e-18, 1.1769073985050732e-18, 3.3192234834681225e-19, 9.361182151543574e-20, 2.640127479072266e-20, 7.445932567440602e-21, 2.0999710144070652e-21, 5.922533169006352e-22, 1.670327776360483e-22, 4.7108134307448195e-23, 1.328587328642571e-23, 3.747005298824538e-24, 1.056765214219347e-24, 2.980387342369092e-25, 8.405565030100302e-26, 2.3706154625033054e-26, 6.685829799087391e-27, 1.885599767645552e-27, 5.317943529940371e-28] . print([np.absolute(item[2][0]) for item in dl]) . [1.147085054406893, 0.16911762309054326, 0.02493343482239269, 0.003675998755668806, 0.0005419617051535398, 7.990277183853153e-05, 1.178026581356281e-05, 1.736794099191309e-06, 2.5605990482090304e-07, 3.775155321370535e-08, 5.565806060282749e-09, 8.205807301561281e-10, 1.2098027265322058e-10, 1.7836424660081822e-11, 2.629668769100653e-12, 3.876986540575399e-13, 5.715938378917869e-14, 8.42715113756738e-15, 1.24243600527847e-15, 1.8317545246125232e-16, 2.7006015977396637e-17, 3.981564395352544e-18, 5.87011978721918e-19, 8.654462461836722e-20, 1.2759506517874804e-20, 1.881157346961903e-21, 2.7734622768821427e-22, 4.0887279184554735e-23, 6.02896728690383e-24, 8.887748720326843e-25, 1.3111499333497393e-25, 1.931131495981037e-26, 2.8525210332791787e-27, 4.165185579566942e-28, 6.626431603856499e-29, 9.466330862652142e-30, 1.9721522630525295e-30, 1.9721522630525295e-31, 9.860761315262648e-32, 2.465190328815662e-32, 2.465190328815662e-32, 0.0, 3.0814879110195774e-33, 1.5407439555097887e-33, 1.1555579666323415e-33, 3.851859888774472e-34, 0.0, 4.81482486096809e-35, 2.407412430484045e-35, 1.2037062152420224e-35, 0.0] . References . I referred to following online resources when I was trying to understand as well as to validate my thinking and used some of the example data from these only to verify the formulas and code. . https://www.cis.upenn.edu/~cis515/cis515-12-sl5.pdf | http://www.robots.ox.ac.uk/~sjrob/Teaching/EngComp/linAlg34.pdf (see page 68 for diagonal dominance) also http://www.robots.ox.ac.uk/~sjrob/Teaching/EngComp/linAlg12.pdf | https://math.unice.fr/~frapetti/CorsoF/cours2.pdf | https://math.unice.fr/~frapetti/CorsoF/cours3.pdf | Diagonal dominance: https://mathworld.wolfram.com/DiagonallyDominantMatrix.html | https://math.la.asu.edu/~gardner/iter.pdf (and https://iuuk.mff.cuni.cz/~rakdver/linalg/lesson15-8.pdf) | https://www-users.cs.umn.edu/~saad/PS/iter2.pdf | https://www-users.math.umn.edu/~olver/num_/lni.pdf | . Online tool to diagonalize: https://www.dcode.fr/matrix-diagonalization (you can copy / paste as mathjax! just need to replace &amp; with &amp; sign) | . $$ begin{array}{l} text{M} to left( begin{array}{ccc} 0 &amp;&amp; -0.5 &amp;&amp; -0.75 0.6 &amp;&amp; 0 &amp;&amp; 0.4 0.25 &amp;&amp; -0.375 &amp;&amp; 0 end{array} right) M=P.D.P^{-1} text{D} to left( begin{array}{ccc} -0.0887509+0.813099 i &amp;&amp; 0. , +0. i &amp;&amp; 0. , +0. i 0. , +0. i &amp;&amp; -0.0887509-0.813099 i &amp;&amp; 0. , +0. i 0. , +0. i &amp;&amp; 0. , +0. i &amp;&amp; 0.177502 , +0. i end{array} right) text{P} to left( begin{array}{ccc} 0.666668 , +0. i &amp;&amp; 0.666668 , +0. i &amp;&amp; -0.551765+0. i -0.228481-0.58075 i &amp;&amp; -0.228481+0.58075 i &amp;&amp; -0.627745+0. i 0.231211 , -0.33559 i &amp;&amp; 0.231211 , +0.33559 i &amp;&amp; 0.549082 , +0. i end{array} right) end{array} $$ .",
            "url": "https://saibaba.github.io/fastpages/2021/07/28/IterativeMethodsForSolvingLinearSystemOfEquations.html",
            "relUrl": "/2021/07/28/IterativeMethodsForSolvingLinearSystemOfEquations.html",
            "date": " • Jul 28, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Floating point numbers",
            "content": "Background . Computer memory can store only discrete values, more specifically a predefined number of 0&#39;s and 1&#39;s for each data type. This also means, all the numbers on a continuous real number line cannot be be represented using predefined size for storage space and we will have to truncate/round extra digits that cannot be fit in the limited space. More specifically, this forces us to limit the range (minimum and maximum values) and density/packing (amount of numbers represented in a given interval) of representable real numbers. . Fixed point notation . One way we could represent real numbers is by using a fixed number of bits for the value before the decimal point and for the value after the decimal point. The problem is that we could be wasting space, and will not able to represent very small or very large numbers. For example consider 2 places for values before decimal point and 2 places for values after decimal point in a decimal number system (ignoring for now that computer actually uses binary number system and each place correspond to one bit of space). You can represent 00.01 to 99.99 nonzero values. The location of decimal point is fixed. . Scientific notation to the rescue . Could we optimize further? For example using scientific notation with the same number of places, we can store the exponent (in a chosen, implicitly assumed number base) and mantissa. Since we are storing exponent (as opposed to the integer part before the decimal point), we can cover a wider range of values. Also, let&#39;s allocate one bit for the sign (0 =&gt; positive number, 1 =&gt; negative number). Concretely, . $(-1)^{sign} * mantissa * { beta}^{e}$ . Here mantissa has a decimal point and we use $ beta$ as the base and $e$ is the exponent. . Taking above example, now you can represent $00.01 x 10^{00}$ to $99.99 x 10^{99}$ (for now, ignore the negative numbers). By allowing negative values for the exponent, you can store very small and large numbers, for example: $0.01 x 10^{-49}$ to $9.99 x 10^{50}$. Notice also that depending on how many places we are using after the decimal point, we loose precision. You could use 2&#39;s complement or come up with a biased representation convention where, for example, you subtract 49 from exponent positions to get actual exponent so that you can represent negative exponents as non-negative numbers for ease. So, 0 to 48 represent negative exponents, 49 is exponent 0 and 50 to 99 represent positive exponents. . To reiterate, with fixed point, you know implicitly where the decimal point is - with floating point, decimal point location is encoded and dynamic (based on the value of exponent). And, very small and large numbers can also be represented. . In both representations, if a number to be represented is not rational like $ sqrt 2$ or that do not have limited digits representation in chosen base (for example 0.1 has recurring digit pattern in binary representation), or has more digits than space allocated for mantissa, not all digits after the decimal point can be represented. So we have to truncate or round off leading to loss precision. More on this later. . Also, notice that since we are using exponent, the gap between the consecutive representable numbers on the real line won&#39;t increase linearly, but exponentially or log-linearly as the exponent increases. . Normalization . Unfortunately, above representation is not unique as we could represent the same number in multiple ways, for example: . $(-1)^{sign} * (mantissa * beta) * { beta}^{e-1}$, here ${mantissa}_{new} = {mantissa}$ * $ beta$ and $e_{new}$ = $e - 1$. . How to deal with this non-unique representations? Normalize! Notice that if we make sure that mantissa is within the range defined by the following relation: . $1 le mantissa lt beta$ . This guarantees that the representation is unique. Why $1 ge$, you may ask. If not, again representation is not unique. For example $0.01 x 10^2 = 0.001 x 10^3$. You already saw an example on why mantissa has to be less than $ beta$. . So, in a normalized floating-point number, the mantissa has one non-zero digit to the left of the decimal point (another way to do this is have a non-zero first digit after the decimal point and only zero before the decimal point). The number zero has no normalized representation as it has no non-zero digit to put just to the left of the decimal point. Any floating-point number that can&#39;t be represented this way is said to be denormalized (for example the exponent becomes smaller than the smallest allowed value after normalizing). . In case where $ beta=2$, the only possible value for the digit left of decimal in mantissa is 1 (for decimal system, the digits are 1 through 9), so it can be effectively implicitly assumed and leave one extra bit available to increase the precision. . Let $E_{min}$ = minimum value representable using given number of slots/bits for exponent. Likewise, for $E_{max}$. . Let p = number of slots/digits in mantissa including the single digit before the decimal point. . Let x be an arbitrary real number and fl(x) its floating point representation. So, if $x = (d_0.d_1 dots d_{p-1} d_p d_{p+1} dots)_ beta * { beta}^e$, then $fl(x) = (d_0.d_1 dots d_{p-1})_ beta * { beta}^e$. In the rest of the document, the base is not shown for brevity. . Then, how do we represent 0? . We can either (a) say that all zeros in mantissa means it is 0 - but then we cannot assume implicit digit. So this reduces the precision (b) use one of the exponent (like $E_{min}$) to indicate 0, reducing the space of available exponents by 1 and keep higher precision. This also preserves that the numerical ordering of nonnegative real numbers corresponds to the lexicographic ordering of their floating-point representations. This option also allows us to compare two numbers by just doing integer comparison of sign bit and exponent bits in integer unit of computer instead of floating point unit. Integer units are faster and cheaper to build them. In the rest of the document, this second convention is adopted as this is the same strategy used by IEEE-754 standard. . What about bias? . Another complication is that the exponent needs to be negative to represent very small numbers. So, it could be represented using 2&#39;s complement. But comparison will be slower. By using a biased representation, it can be made faster. You saw an example above where we subtracted 49. In the rest of the document, this biased representation convention is adopted as this is the same strategy used by IEEE-754 standard. . Subnormal numbers . So, if we represent 0 with all zeros in exponent, we have a decision about the values in mantissa? We can say (1) all zeros in mantissa means the we are representing 0 (2) non zero mantissa means, it is denormalized number (do not assume implicit 1 in case of binary). This second option is useful. Why? . The gap between 0 and the smallest normalized number is ($ beta^{E_{min}+1}$) greater than the gap between smallest normalized number and the next bigger floating point number ($ beta^{E_{min}+1-p+1}$). So, in general, even though the gap is non-increasing as we go from higher numbers to lower numbers, this rule is violated between the smallest normalized number and 0. So, this requires special case handling in proofs for this range or exclude certain floating point numbers from the proofs. Also, if the result of an operation (like subtracting two very small numbers whose exponent is already minimum allowed) cannot be normalized, it would have to be flushed to zero, an abrupt change. What can we do about this? If we allow denormalized numbers, we can assert that if the difference between $x_0$ and its nearest floating point is, say $u$, then, for any $x le x_0$, the difference between x and its nearest floating point is at most $u$. Then you can go on and conclude that the error produced by some sequence of computations is at most some value as long as the input is within bounds. Also, they help to underflow gradually. See slide 25 of http://www.cas.mcmaster.ca/~qiao/courses/cas708/slides/ch01.pdf for a picture. . Underflow and overflow . Given that we are using a fixed number of bits/places for exponent, you have a $E_{min}$ and a $E_{max}$ value that exponent can take. Underflow is the situation where numbers whose exponent is below the $E_{min}+1$ (remember, $E_{min}$ is already used to represent 0) exponent and hence cannot be represented (mostly as a result of an operation like subtracting two close numbers). Overflow is about the numbers whose exponent is above the $E_{max}-1$ (remember, $E_{max}$ is already used for $+- infty$) exponent value and hence cannot be represented as well. Note that each kind of numbers can be positive or negative depending on the sign bit/place. . Subnormal numbers can help with gradual underflow. Consider for example, $x = 1.10 x 10^{E_{min}+1}$, and $y = 1.00 x 10^{E_{min}+1}$. Then $x-y = 0.1 x 10^{E_{min}+1} = 1.0 x 10^{E_{min}}$. The exponent is smaller than smallest representable value, hence result will be $0$. So, even though $x ne y$, $x-y = 0$. To handle these cases, we could use denormalized/subnormal numbers to represent the underflow numbers (above will be $0.1 x 10^{E_{min}}$ i.e., do not constrain that digit before the decimal point &gt; 1). This also means we do not assume implicit 1, in case of binary when exponent = ${E_{min}}$. Since there could be many zeros after the decimal point, we lose precision (we have to truncate/round the small number to fit in the bits/place of mantissa size). Another example $a = 3.0 × 2^{-64}$ and $a*a$ is too small to be represented in 32 bit float format (IEEE-754). . Also, notice that when we subtract two nearly equal quantities (and hence have matching digits for a great number of positions in mantissa), there will be significant loss of precision due to these large number of zeros in the fraction/mantissa. . How to represent special numbers . $ infty$, $- infty$ ? We can have a convention that all 1&#39;s (in binary, or 9&#39;s in decimal) in exponent represent $ infty$. The content of mantissa in this case is 0. This is because, by convention we say that max value represented is $ beta^{E_{max}}$ and non-zero mantissa makes the number value greater than this (i.e., $ infty$) which is meaningless. . We still need to address another aspect: Any number, with the exception of zero, divided by zero yields respectively $ infty$ or $- infty$. Dividing zero with zero results in the special NaN, the Not a Number value. NaN is useful to represent situations like addition of two infinite numbers. So, how do we represent these? Notice that when representing $ infty$ we assumed all 0&#39;s for mantissa. So, we can use any non-zero as meaning it is a NaN. . An example . Now background is out of our way, let&#39;s see an example of how floating points are represented in computer to get a concrete idea. . We first pick how many bits we want to use to represent decimal numbers, say 8 bits. . First bit (MSB) can be used for sign: 0(+ve), 1(-ve) Next 3 bits can be used for exponent. Remaining 4 bits can be used for mantissa. . Let&#39;s use biased representation, so exponent value stored ranges from 0 to 7 (unsigned +ve number) with a bias of 3. So, actual values for exponent ranges from $E_{min} = -3$ to $E_{max} = 4$. . Let&#39;s also say that exponent = 000 with mantissa = 0000 is reserved for 0. . Let&#39;s say exponent = 111 mantissa = 0000 reserved for INFINITY. . This will rule out -3 and 4 from being used as exponents for normal numbers. So, actual represented range is from 1 to 6 (biased representation) or as values from -2 to 3. . So, we now have special cases: . exponent = 000 with mantissa != 0000 =&gt; subnormal numbers . exponent = 111 with mantissa != 0000 =&gt; NaN (Any number, with the exception of zero, divided by zero yields respectively ∞ or -∞. Dividing zero with zero results in the special NaN, the Not a Number value). NaN is useful to represent situations like addition of two infinites, $ frac{0}{0}$. . max +ve value: . 0 110 1111 . max -ve value: . 1 110 1111 . Smallest +ve number that can be represented: . 0 000 0001 (subnormal, note that mantissa cannot be 0000 as then it becomes 0). . Smallest -ve number: . 1 000 0001 (subnormal) . Smallest normal +ve number: . 0 001 0000 . Smallest normal -ve number: . 1 001 0000 . Gap between 0 and smallest normal +ve number = $2^{1-3} = 0.25$. Gap between the smallest normal +ve number and the number next = &lt;0 001 0001&gt; - $2^{1-3}$ = $1.0001 x 2^{1-3} - 1.0000 x 2^{1-3} = 0.0001 x 2^{1-3} = 2^{1-3-4} = 0.015625$. . So, we notice that floating point representation is discrete (unlike real line), not equally spaced throughout, and finite. . Measuring rounding errors . Note that we are representing a real number using a fixed width storage (sign+mantissa+exponent) and hence we have a rounding error. This is measured using ULP standing for units in the last place. For a real number x, when represented using a floating point number, say, fl(x), with exponent e, a least possible change in the mantissa modifies the represented value fl(x) by ${ beta}^e * { beta}^{-p+1} = { beta}^{e-p+1}$. This is characterized as 1 ULP. So, all floating point numbers with the same exponent, $e$ have the same ULP. So, in a given representation (given radix, precision, base), 1 ULP is different for each value of the exponent and not constant throughout the representable range. . How do we round? there are four types of rounding we can do. Here they are with some examples (where we assume to keep 5 digits after decimal point): . Round to nearest number representable: e.g. -0.001497 becomes -0.00150. | Round to zero/truncate: e.g. -0.001498 becomes -0.00149. | Round to +infinity (round up): e.g. -0.001498 becomes -0.00149. | Round to –infinity (round down): e.g. -0.001498 becomes -0.00150. | . How can we ensure the rounding is done correctly? particularly given that we cannot store digits after the lowest possible slot/digit? Use an extra digit(s) called guard digit(s). Then calculations are performed at slightly greater precision, and then stored in standard IEEE floating-point numbers after normalizing and rounding as above. Usually three extra bits are enough to ensure correctness. . ULP is good for measuring rounding error. Relative error is good for measuring rounding error due to various formula (add/subtract etc). . Rounding to the nearest floating-point number corresponds to an error of less than or equal to 0.5 ULP. However, when analyzing the rounding error caused by various formulas, relative error is a better measure as it is not affected by exponent. . Some proofs . Rounding error introduced by nearest rounding . $ beta = radix$ as above. . let $m = beta - 1$ . let $h = frac{ beta}{2}$ . Then, $0.h = frac{ beta}{2} { beta}^{-1} = frac{1}{2}$ . $x = d_0.d_1 dots d_{p-1} d_p d_{p+1} dots { beta}^e$ . say $d_p lt h$, then we just truncate . $fl(x) = d_0.d_1 dots d_{p-1} { beta}^e$ . $x - fl(x) = 0. dots d_{p} d_{p+1} dots { beta}^e$ . $x - fl(x) = 0.d_{p} d_{p+1} dots { beta}^{-p+1} { beta}^e$ . $x - fl(x) le 0.h m m m dots { beta}^{-p+1} { beta}^e$ . $x - fl(x) le 0.h { beta}^{-p+1} { beta}^e$ . $x - fl(x) le frac{1}{2} { beta}^{-p+1+e}$ . Let $d_p ge h$, then we add $ frac{ beta}{2} beta^{e-p} = frac{1}{2} beta^{e-p+1}$ and truncate. . $fl(x) = d_0.d_1 dots d_{p-1} beta^e + frac{1}{2} beta^{e-p+1}$ . $x - fl(x) = 0.0 dots d_{p} d_{p+1} dots { beta}^e - frac{1}{2} beta^{e-p+1}$ . $x - fl(x) = 0.d_{p} d_{p+1} dots { beta}^{e-p+1} - frac{1}{2} beta^{e-p+1}$ . $x - fl(x) le 0.m m dots { beta}^{e-p+1} - frac{1}{2} beta^{e-p+1}$ . $x - fl(x) lt 1.0 { beta}^{e-p+1} - frac{1}{2} beta^{e-p+1}$ . $x - fl(x) lt frac{1}{2} { beta}^{-p+1+e}$ . Also, ${ beta}^{-p+1+e}$ = ulp(x), so rounding to nearest floating point keeps absolute error within half-ulp. . Consider relative error, . $ frac{|x-fl(x)|}{|fl(x)|} lt frac{ frac{1}{2} { beta}^{-p+1+e}}{ beta^e} = frac{1}{2} beta^{-p+1}$ . (here note that $|fl(x)| ≥ 1.0 × beta^e$} . Relative error is indpendent of exponent, hence the same for all numbers. . Rounding error introduced by pure truncation is twice as much. Why? . $fl(x) = d_0.d_1 dots d_{p-1} { beta}^e$ . $x - fl(x) = 0. dots d_{p} d_{p+1} dots { beta}^e$ . $x - fl(x) le 0.m m dots { beta}^{e-p+1}$ . $x - fl(x) lt 1.0 dots { beta}^{e-p+1}$ . Operations (subtraction etc.,) using fixed bits in hardware lead to high rounding errors (the ulp between actual result vs. computed is high). Using a guard digit helps. But there is also problem of cancellation. And sometimes exact rounding (i.e,, assume infinite precision bits while operating, and round the result afterwards) is needed. . Arithmetic exceptions . Overflow condition: ±$ infty$, f = $1111 dots$ . Underflow condition: flush to 0, ±$2^{-bias}$, [denormalized] . Divide by zero: ±$ infty$ . Invalid numbers: NaN . Inexact value due to rounding/truncation: arithmetic operations . Obviously inexact would occur very often and is usually ignored. So is the case with underflow. We may want to catch the remaining exceptions. . Sample code to demonstrate some of the concepts . from decimal import * getcontext().prec = 28 # A class for creating floating points with given number of bits for exponent and precision class MyFloat: def __init__(self, exp_bits, precision): self.exp_bits = exp_bits self.mantissa_bits = precision - 1 # implicit 1 self.bias = (2**(self.exp_bits - 1)) - 1 self.max_exp_value = (2**self.exp_bits) - 1 def print_float(f): print(bin(f)) def create(self, s, e, m): return (s &lt;&lt; (self.exp_bits + self.mantissa_bits) | (e &lt;&lt; self.mantissa_bits) | m) def sign(self, f): return f &gt;&gt; (self.exp_bits + self.mantissa_bits) def exp(self, f): return ((f &gt;&gt; self.mantissa_bits) &amp; (2**self.exp_bits -1)) def mantissa(self, f): return f &amp; ((2**self.mantissa_bits) - 1) def fraction(self, f): fracs = [] for i in range(0, self.mantissa_bits): e = 2**(self.mantissa_bits-i) fracs.append( (f &amp; 1)/e ) f = f &gt;&gt; 1 return sum(fracs) def _epsilon(self): return -self.mantissa_bits def ulp(self, n): e = self.exp(n) s = self.sign(n) m = self.mantissa(n) r = self._special(e, s, m) if r is not None: return r e = e - self.bias return &quot;2**&quot; + str(self._epsilon() + e) def as_binary(self, f): # TODO: handle subnormal and special numbers s = 0 if f &lt; 0: s = 1 f = -f e = 0 while f &lt; 1: f *= 2 e -= 1 while f &gt; 2: f = f/2 e += 1 e = e + self.bias f -= 1 # implicit 1 mbits = [] if (f != 0): while True: f *= 2 if f &gt; 1: f -= 1 mbits.append(&quot;1&quot;) elif f &lt; 1: mbits.append(&quot;0&quot;) else: mbits.append(&quot;1&quot;) break if (len(mbits) == self.mantissa_bits): break txt = &quot;{0} &quot; + &quot;{0:b}&quot;.format(e).zfill(self.exp_bits) + &quot; &quot; + &quot;&quot;.join(mbits).zfill(self.mantissa_bits) return txt.format(s) def _special(self, e, s, m): if (e == self.max_exp_value) and (m == 0) and (s == 0): return &quot;+Infinity&quot; if (e == self.max_exp_value) and (m == 0) and (s == 1): return &quot;-Infinity&quot; if (e == self.max_exp_value and s == 0): return &quot;NaN&quot; if (e == self.max_exp_value and s == 1): return &quot;-NaN&quot; return None def as_decimal(self, n): e = self.exp(n) s = self.sign(n) m = self.mantissa(n) r = self._special(e, s, m) if r is not None: return r frac = self.fraction(m) if e &gt; 0 and e &lt; self.max_exp_value: # normalized numbers, add implicit 1 frac += 1.0 e -= self.bias if e == -self.bias: e += 1 # subnormal numbers, representable numbers which are immediately close to smallest normal number # additionally, if we keep e=-3, then numbers smaller than smallest are represented causing confusion # in this case we also cannot use assumed b0 = 1 as that leads to duplicate numbers, for example then one with when e=-2 already return Decimal((-1)**self.sign(n)) * Decimal(2**e) * Decimal(frac) . myFloat32 = MyFloat(8, 24) def print_details(n): print(bin(myFloat32.sign(n))) print(bin(myFloat32.exp(n))) print(bin(myFloat32.mantissa(n))) print(myFloat32.mantissa_bits) print(myFloat32.as_decimal(n)) print(myFloat32.ulp(n)) #print_details(0x3E800000) #print_details(0x3E1BA5E3) # 1.5199999511241912841796875E-1 #print_details(0x3F7F7CEE) # 9.9800002574920654296875E-1 print_details(0x4480C000) # 1030.000000000 print(&quot;0.25 =&gt; &quot; + myFloat32.ulp(0x3E800000)) print(&quot;2 =&gt; &quot; + myFloat32.ulp(0x40000000)) print(&quot;3 =&gt; &quot; + myFloat32.ulp(0x40400000)) print(&quot;4 =&gt; &quot; + myFloat32.ulp(0x40800000)) print(&quot;10 =&gt; &quot; + myFloat32.ulp(0x41200000)) print(&quot;100 =&gt; &quot; + myFloat32.ulp(0x42C80000)) print(&quot;1030 =&gt; &quot; + myFloat32.ulp(0x4480C000)) print(&quot;1030 =&gt; &quot; + myFloat32.ulp(0x4480C000)) #verified above are correct Ulp by testing through https://docs.oracle.com/javase/1.5.0/docs/api/java/lang/Math.html#ulp(float) print(&quot;Bits for 0.25 =&quot;, myFloat32.as_binary(0.25)) print(&quot;Bits for 0.1 =&quot;, myFloat32.as_binary(0.1)) . 0b0 0b10001001 0b1100000000000000 23 1030.000000000 2**-13 0.25 =&gt; 2**-25 2 =&gt; 2**-22 3 =&gt; 2**-22 4 =&gt; 2**-21 10 =&gt; 2**-20 100 =&gt; 2**-17 1030 =&gt; 2**-13 1030 =&gt; 2**-13 Bits for 0.25 = 0 01111101 00000000000000000000000 Bits for 0.1 = 0 01111011 10011001100110011001100 . Sample floating point number system . Create a floating point number type with 1 bit for sign, 8 bits for exponent, and 5 bits (1 implicit and 4 explicit) for mantissa . myFloat = MyFloat(3, 5) print(&quot;Epsilon&quot; + str(myFloat._epsilon())) xs = [] ys = [] for i in range(0, 256): xs.append(i) print(&quot;N =&quot;, i, &quot;-&gt; &quot;, end=&#39;&#39;) y = myFloat.as_decimal(i) print(y) if (i &gt; 111 and i &lt; 128) or (i &gt; 239 and i &lt; 256): ys.append(0) else: ys.append(y) . Epsilon-4 N = 0 -&gt; 0.00 N = 1 -&gt; 0.015625 N = 2 -&gt; 0.03125 N = 3 -&gt; 0.046875 N = 4 -&gt; 0.0625 N = 5 -&gt; 0.078125 N = 6 -&gt; 0.09375 N = 7 -&gt; 0.109375 N = 8 -&gt; 0.125 N = 9 -&gt; 0.140625 N = 10 -&gt; 0.15625 N = 11 -&gt; 0.171875 N = 12 -&gt; 0.1875 N = 13 -&gt; 0.203125 N = 14 -&gt; 0.21875 N = 15 -&gt; 0.234375 N = 16 -&gt; 0.25 N = 17 -&gt; 0.265625 N = 18 -&gt; 0.28125 N = 19 -&gt; 0.296875 N = 20 -&gt; 0.3125 N = 21 -&gt; 0.328125 N = 22 -&gt; 0.34375 N = 23 -&gt; 0.359375 N = 24 -&gt; 0.375 N = 25 -&gt; 0.390625 N = 26 -&gt; 0.40625 N = 27 -&gt; 0.421875 N = 28 -&gt; 0.4375 N = 29 -&gt; 0.453125 N = 30 -&gt; 0.46875 N = 31 -&gt; 0.484375 N = 32 -&gt; 0.5 N = 33 -&gt; 0.53125 N = 34 -&gt; 0.5625 N = 35 -&gt; 0.59375 N = 36 -&gt; 0.625 N = 37 -&gt; 0.65625 N = 38 -&gt; 0.6875 N = 39 -&gt; 0.71875 N = 40 -&gt; 0.75 N = 41 -&gt; 0.78125 N = 42 -&gt; 0.8125 N = 43 -&gt; 0.84375 N = 44 -&gt; 0.875 N = 45 -&gt; 0.90625 N = 46 -&gt; 0.9375 N = 47 -&gt; 0.96875 N = 48 -&gt; 1 N = 49 -&gt; 1.0625 N = 50 -&gt; 1.125 N = 51 -&gt; 1.1875 N = 52 -&gt; 1.25 N = 53 -&gt; 1.3125 N = 54 -&gt; 1.375 N = 55 -&gt; 1.4375 N = 56 -&gt; 1.5 N = 57 -&gt; 1.5625 N = 58 -&gt; 1.625 N = 59 -&gt; 1.6875 N = 60 -&gt; 1.75 N = 61 -&gt; 1.8125 N = 62 -&gt; 1.875 N = 63 -&gt; 1.9375 N = 64 -&gt; 2 N = 65 -&gt; 2.1250 N = 66 -&gt; 2.250 N = 67 -&gt; 2.3750 N = 68 -&gt; 2.50 N = 69 -&gt; 2.6250 N = 70 -&gt; 2.750 N = 71 -&gt; 2.8750 N = 72 -&gt; 3.0 N = 73 -&gt; 3.1250 N = 74 -&gt; 3.250 N = 75 -&gt; 3.3750 N = 76 -&gt; 3.50 N = 77 -&gt; 3.6250 N = 78 -&gt; 3.750 N = 79 -&gt; 3.8750 N = 80 -&gt; 4 N = 81 -&gt; 4.2500 N = 82 -&gt; 4.500 N = 83 -&gt; 4.7500 N = 84 -&gt; 5.00 N = 85 -&gt; 5.2500 N = 86 -&gt; 5.500 N = 87 -&gt; 5.7500 N = 88 -&gt; 6.0 N = 89 -&gt; 6.2500 N = 90 -&gt; 6.500 N = 91 -&gt; 6.7500 N = 92 -&gt; 7.00 N = 93 -&gt; 7.2500 N = 94 -&gt; 7.500 N = 95 -&gt; 7.7500 N = 96 -&gt; 8 N = 97 -&gt; 8.5000 N = 98 -&gt; 9.000 N = 99 -&gt; 9.5000 N = 100 -&gt; 10.00 N = 101 -&gt; 10.5000 N = 102 -&gt; 11.000 N = 103 -&gt; 11.5000 N = 104 -&gt; 12.0 N = 105 -&gt; 12.5000 N = 106 -&gt; 13.000 N = 107 -&gt; 13.5000 N = 108 -&gt; 14.00 N = 109 -&gt; 14.5000 N = 110 -&gt; 15.000 N = 111 -&gt; 15.5000 N = 112 -&gt; +Infinity N = 113 -&gt; NaN N = 114 -&gt; NaN N = 115 -&gt; NaN N = 116 -&gt; NaN N = 117 -&gt; NaN N = 118 -&gt; NaN N = 119 -&gt; NaN N = 120 -&gt; NaN N = 121 -&gt; NaN N = 122 -&gt; NaN N = 123 -&gt; NaN N = 124 -&gt; NaN N = 125 -&gt; NaN N = 126 -&gt; NaN N = 127 -&gt; NaN N = 128 -&gt; -0.00 N = 129 -&gt; -0.015625 N = 130 -&gt; -0.03125 N = 131 -&gt; -0.046875 N = 132 -&gt; -0.0625 N = 133 -&gt; -0.078125 N = 134 -&gt; -0.09375 N = 135 -&gt; -0.109375 N = 136 -&gt; -0.125 N = 137 -&gt; -0.140625 N = 138 -&gt; -0.15625 N = 139 -&gt; -0.171875 N = 140 -&gt; -0.1875 N = 141 -&gt; -0.203125 N = 142 -&gt; -0.21875 N = 143 -&gt; -0.234375 N = 144 -&gt; -0.25 N = 145 -&gt; -0.265625 N = 146 -&gt; -0.28125 N = 147 -&gt; -0.296875 N = 148 -&gt; -0.3125 N = 149 -&gt; -0.328125 N = 150 -&gt; -0.34375 N = 151 -&gt; -0.359375 N = 152 -&gt; -0.375 N = 153 -&gt; -0.390625 N = 154 -&gt; -0.40625 N = 155 -&gt; -0.421875 N = 156 -&gt; -0.4375 N = 157 -&gt; -0.453125 N = 158 -&gt; -0.46875 N = 159 -&gt; -0.484375 N = 160 -&gt; -0.5 N = 161 -&gt; -0.53125 N = 162 -&gt; -0.5625 N = 163 -&gt; -0.59375 N = 164 -&gt; -0.625 N = 165 -&gt; -0.65625 N = 166 -&gt; -0.6875 N = 167 -&gt; -0.71875 N = 168 -&gt; -0.75 N = 169 -&gt; -0.78125 N = 170 -&gt; -0.8125 N = 171 -&gt; -0.84375 N = 172 -&gt; -0.875 N = 173 -&gt; -0.90625 N = 174 -&gt; -0.9375 N = 175 -&gt; -0.96875 N = 176 -&gt; -1 N = 177 -&gt; -1.0625 N = 178 -&gt; -1.125 N = 179 -&gt; -1.1875 N = 180 -&gt; -1.25 N = 181 -&gt; -1.3125 N = 182 -&gt; -1.375 N = 183 -&gt; -1.4375 N = 184 -&gt; -1.5 N = 185 -&gt; -1.5625 N = 186 -&gt; -1.625 N = 187 -&gt; -1.6875 N = 188 -&gt; -1.75 N = 189 -&gt; -1.8125 N = 190 -&gt; -1.875 N = 191 -&gt; -1.9375 N = 192 -&gt; -2 N = 193 -&gt; -2.1250 N = 194 -&gt; -2.250 N = 195 -&gt; -2.3750 N = 196 -&gt; -2.50 N = 197 -&gt; -2.6250 N = 198 -&gt; -2.750 N = 199 -&gt; -2.8750 N = 200 -&gt; -3.0 N = 201 -&gt; -3.1250 N = 202 -&gt; -3.250 N = 203 -&gt; -3.3750 N = 204 -&gt; -3.50 N = 205 -&gt; -3.6250 N = 206 -&gt; -3.750 N = 207 -&gt; -3.8750 N = 208 -&gt; -4 N = 209 -&gt; -4.2500 N = 210 -&gt; -4.500 N = 211 -&gt; -4.7500 N = 212 -&gt; -5.00 N = 213 -&gt; -5.2500 N = 214 -&gt; -5.500 N = 215 -&gt; -5.7500 N = 216 -&gt; -6.0 N = 217 -&gt; -6.2500 N = 218 -&gt; -6.500 N = 219 -&gt; -6.7500 N = 220 -&gt; -7.00 N = 221 -&gt; -7.2500 N = 222 -&gt; -7.500 N = 223 -&gt; -7.7500 N = 224 -&gt; -8 N = 225 -&gt; -8.5000 N = 226 -&gt; -9.000 N = 227 -&gt; -9.5000 N = 228 -&gt; -10.00 N = 229 -&gt; -10.5000 N = 230 -&gt; -11.000 N = 231 -&gt; -11.5000 N = 232 -&gt; -12.0 N = 233 -&gt; -12.5000 N = 234 -&gt; -13.000 N = 235 -&gt; -13.5000 N = 236 -&gt; -14.00 N = 237 -&gt; -14.5000 N = 238 -&gt; -15.000 N = 239 -&gt; -15.5000 N = 240 -&gt; -Infinity N = 241 -&gt; -NaN N = 242 -&gt; -NaN N = 243 -&gt; -NaN N = 244 -&gt; -NaN N = 245 -&gt; -NaN N = 246 -&gt; -NaN N = 247 -&gt; -NaN N = 248 -&gt; -NaN N = 249 -&gt; -NaN N = 250 -&gt; -NaN N = 251 -&gt; -NaN N = 252 -&gt; -NaN N = 253 -&gt; -NaN N = 254 -&gt; -NaN N = 255 -&gt; -NaN . import numpy as np import matplotlib.pyplot as plt from matplotlib.pyplot import figure figure(figsize=(16, 2), dpi=120) values = ys[0:112] plt.scatter(values, np.zeros_like(values), cmap=&quot;hot_r&quot;, vmin=-2) plt.xticks(np.linspace(0, 16, 20)) plt.yticks([]) plt.show() . import numpy as np import matplotlib.pyplot as plt lys = [] for i in range(16, 112): ly = myFloat.as_decimal(i) lys.append(ly) plt.hist(lys, &#39;auto&#39;, facecolor=&#39;g&#39;, alpha=0.75) plt.xlabel(&#39;floating point range&#39;) plt.ylabel(&#39;count&#39;) plt.title(&#39;Histogram showing floating point packing&#39;) plt.grid(True) plt.show() . import numpy as np import matplotlib.pyplot as plt plt.hist([np.log(float(x)) for x in lys], &#39;auto&#39;, facecolor=&#39;g&#39;, alpha=0.75) plt.xlabel(&#39;log-floating point range&#39;) plt.ylabel(&#39;count&#39;) plt.title(&#39;Histogram showing log-linear packing of floating point numbers&#39;) plt.grid(True) plt.show() . Exercises . Understand half precision float vs bfloat16 representation | Demonstrate how the knowledge of floating point representation is useful: Convert from float to bfloat16. Execute an example arithmetic expression using both half-float and bfloat16 and see the difference in results and explain. | . | Fast square root by halving exponent directly | . | Machine epsilon: when two successive iterates differ by less than $| epsilon|$, we may assume that the iteration has converged and stop the process. Provide a demonstration of this. | . References . https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html | https://www.exploringbinary.com/the-spacing-of-binary-floating-point-numbers/ | https://softwareengineering.stackexchange.com/questions/215065/can-anyone-explain-representation-of-float-in-memory | https://stackoverflow.com/questions/55253233/convert-fp32-to-bfloat16-in-c/55254307#55254307 | https://software.intel.com/content/www/us/en/develop/articles/intel-deep-learning-boost-new-instruction-bfloat16.html | http://www.binaryconvert.com/result_float.html?decimal=048046049053054050053 | http://www.cs.utep.edu/interval-comp/hayes.pdf | https://stackoverflow.com/questions/40082459/what-is-overflow-and-underflow-in-floating-point | Floating point vs. bigint: https://stackoverflow.com/a/6320218 | http://www.cs.jhu.edu/~jorgev/cs333/readings/8-Bit_Floating_Point.pdf | https://babbage.cs.qc.cuny.edu/IEEE-754.old/Decimal.html | https://www.youtube.com/watch?v=p8u_k2LIZyo | https://www-ljk.imag.fr/membres/Carine.Lucas/TPScilab/JMMuller/ulp-toms.pdf | https://matthew-brett.github.io/teaching/floating_error.html | http://www.math.pitt.edu/~trenchea/math1070/MATH1070_2_Error_and_Computer_Arithmetic.pdf | http://home.iitk.ac.in/~pranab/ESO208/rajesh/03-04/Errors.pdf | https://stackoverflow.com/a/7524916 | http://www.cas.mcmaster.ca/~qiao/courses/cas708/slides/ch01.pdf | https://stackoverflow.com/questions/43965347/ulp-unit-of-least-precision | Algorithms for standard operations: https://www.rfwireless-world.com/Tutorials/floating-point-tutorial.html | https://www.sciencedirect.com/topics/computer-science/fixed-point-number | https://stackoverflow.com/questions/51170944/understanding-the-usefulness-of-denormalized-floating-point-numbers | https://www.ias.ac.in/public/Volumes/reso/021/01/0011-0030.pdf | .",
            "url": "https://saibaba.github.io/fastpages/floating/point/2021/07/27/FloatingPointNumbers.html",
            "relUrl": "/floating/point/2021/07/27/FloatingPointNumbers.html",
            "date": " • Jul 27, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://saibaba.github.io/fastpages/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://saibaba.github.io/fastpages/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Sai",
          "content": "",
          "url": "https://saibaba.github.io/fastpages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://saibaba.github.io/fastpages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}