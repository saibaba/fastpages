{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Floating point numbers\"\n",
    "> \"Understand computer representation of real numbers\"\n",
    "\n",
    "- toc:true\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Saibaba Telukunta\n",
    "- categories: [floating, point]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqBaWMY3txYs"
   },
   "source": [
    "Background\n",
    "---\n",
    "\n",
    "Computer memory can store only discrete values, more specifically a predefined number of 0's and 1's for each data type. This also means, all the numbers on a continuous real number line cannot be be represented using predefined size for storage space and we will have to truncate/round extra digits that cannot be fit in the limited space. More specifically, this forces us to limit the range (minimum and maximum values) and density/packing (amount of numbers represented in a given interval) of representable real numbers.\n",
    "\n",
    "**Fixed point notation**\n",
    "\n",
    "One way we could represent real numbers is by using a fixed number of bits for the value before the decimal point and for the value after the decimal point. The problem is that we could be wasting space, and will not able to represent very small or very large numbers. For example consider 2 places for values before decimal point and 2 places for values after decimal point in a decimal number system (ignoring for now that computer actually uses binary number system and each place correspond to one bit of space). You can represent 00.01 to 99.99 nonzero values. The location of decimal point is fixed.\n",
    "\n",
    "**Scientific notation to the rescue**\n",
    "\n",
    "Could we optimize further? For example using scientific notation with the same number of places, we can store the exponent (in a chosen, implicitly assumed number base) and mantissa. Since we are storing exponent (as opposed to the integer part before the decimal point), we can cover a wider range of values. Also, let's allocate one bit for the sign (0 => positive number, 1 => negative number). Concretely,\n",
    "\n",
    "$(-1)^{sign} * mantissa * {\\beta}^{e}$\n",
    "\n",
    "Here `mantissa` has a decimal point and we use $\\beta$ as the base and $e$ is the exponent.\n",
    "\n",
    "Taking above example now you can represent $0.0 * 10^{00}$ to $9.9 * 10^{99}$ (for brevity, ignoring the negative numbers, and assuming we keep one place value before the decimal point and another for the digit after the decimal point. And two digits for the exponent. A total of 4 digits). By allowing negative values for the exponent, you can store very small and large numbers. In above example, smallest nonzero number $0.1 * 10^{-49}$ to $9.9 * 10^{50}$. You could use 2's complement or come up with a biased representation convention where, for example, you subtract 49 from exponent positions to get actual exponent so that you can represent negative exponents as non-negative numbers for ease. So, 0 to 48 represent negative exponents, 49 is exponent 0 and 50 to 99 represent positive exponents. Also, notice also that depending on how many places we are using after the decimal point, we loose precision. \n",
    "\n",
    "To reiterate, with fixed point, you know implicitly where the decimal point is - with floating point, decimal point location is encoded and dynamic (based on the value of exponent). And, very small and large numbers can also be represented.\n",
    "\n",
    "In both representations, if a number to be represented is not rational like $\\sqrt 2$ or that do not have limited digits representation in chosen base (for example 0.1 has recurring digit pattern in binary representation), or has more digits than space allocated for `mantissa`, not all digits after the decimal point can be represented. So we have to truncate or round off leading to loss `precision`. More on this later.\n",
    "\n",
    "Also, notice that since we are using exponent, the gap between the consecutive representable numbers on the real line won't increase linearly, but exponentially or log-linearly as the exponent increases.\n",
    "\n",
    "**Normalization**\n",
    "\n",
    "Unfortunately, above representation is not unique as we could represent the same number in multiple ways, for example:\n",
    "\n",
    "$(-1)^{sign} * (mantissa * \\beta) * {\\beta}^{e-1}$, here ${mantissa}_{new} = {mantissa}$ * $\\beta$ and $e_{new}$ = $e - 1$.\n",
    "\n",
    "How to deal with this non-unique representations? Normalize! Notice that if we make sure that `mantissa` is within the range defined by the following relation:\n",
    "\n",
    "$1 \\le mantissa \\lt \\beta$\n",
    "\n",
    "This guarantees that the representation is unique. Why $1 \\ge$, you may ask. If not, again representation is not unique. For example $0.01 * 10^2 = 0.001 * 10^3$. You already saw an example on why `mantissa` has to be less than $\\beta$.\n",
    "\n",
    "So, in a normalized floating-point number, the mantissa has one non-zero digit to the left of the decimal point (another way to do this is have a non-zero first digit after the decimal point and only zero before the decimal point). The number zero has no normalized representation as it has no non-zero digit to put just to the left of the decimal point. Any floating-point number that can't be represented this way is said to be denormalized (for example the exponent becomes smaller than the smallest allowed value after normalizing).\n",
    "\n",
    "In case where $\\beta=2$, the only possible value for the digit left of decimal in mantissa is `1` (for decimal system, the digits are `1` through `9`), so it can be effectively *implicitly* assumed and leave one extra bit available to increase the precision.\n",
    "\n",
    "Let $E_{min}$ = minimum value representable using given number of slots/bits for exponent. Likewise, for $E_{max}$. \n",
    "\n",
    "Let p = number of slots/digits in `mantissa` including the single digit before the decimal point.\n",
    "\n",
    "Let x be an arbitrary real number and fl(x) its floating point representation. So, if $x = (d_0.d_1 \\dots d_{p-1} d_p d_{p+1} \\dots)_\\beta * {\\beta}^e$, then $fl(x) = (d_0.d_1 \\dots d_{p-1})_\\beta * {\\beta}^e$. In the rest of the document, the base is not shown for brevity.\n",
    "\n",
    "**Then, how do we represent 0?**\n",
    "\n",
    "We can either (a) say that all zeros in `mantissa` means it is 0 - but then we cannot assume implicit digit. So this reduces the precision (b) use one of the exponent (like $E_{min}$) to indicate 0, reducing the space of available exponents by 1 and keep higher precision. This also preserves that the numerical ordering of nonnegative real numbers corresponds to the lexicographic ordering of their floating-point representations. This option also allows us to compare two numbers by just doing integer comparison of sign bit and exponent bits in integer unit of computer instead of floating point unit. Integer units are faster and cheaper to build them. In the rest of the document, this second convention is adopted as this is the same strategy used by IEEE-754 standard.\n",
    "\n",
    "**What about bias?**\n",
    "\n",
    "Another complication is that the exponent needs to be negative to represent very small numbers. So, it could be represented using 2's complement. But comparison will be slower. By using a biased representation, it can be made faster. You saw an example above where we subtracted 49. In the rest of the document, this biased representation convention is adopted as this is the same strategy used by IEEE-754 standard.\n",
    "\n",
    "**Subnormal numbers**\n",
    "\n",
    "So, if we represent 0 with all zeros in `exponent`, we have a decision about the values in `mantissa`? We can say (1) all zeros in `mantissa` means the we are representing 0 (2) non zero `mantissa` means, it is denormalized number (do not assume *implicit* 1 in case of binary). This second option is useful. Why?\n",
    "\n",
    "The gap between 0 and the smallest normalized number is ($\\beta^{E_{min}+1}$)  greater than the gap between smallest normalized number and the next bigger floating point number ($\\beta^{E_{min}+1-p+1}$). So, in general, even though the gap is non-increasing as we go from higher numbers to lower numbers, this rule is violated between the smallest normalized number and 0. So, this requires special case handling in proofs for this range or exclude certain floating point numbers from the proofs. Also, if the result of an operation (like subtracting two very small numbers whose exponent is already minimum allowed) cannot be normalized, it would have to be flushed to zero, an abrupt change.  What can we do about this? If we allow denormalized numbers, we can assert that if the difference between $x_0$ and its nearest floating point is, say $u$, then, for any $x \\le x_0$, the difference between x and its nearest floating point is at most $u$. Using this fact in proofs, we can conclude that the error produced by some sequence of computations is at most some value. Also, they help to underflow gradually. See slide 25 of http://www.cas.mcmaster.ca/~qiao/courses/cas708/slides/ch01.pdf for a picture.\n",
    "\n",
    "\n",
    "**Underflow and overflow**\n",
    "\n",
    "Given that we are using a fixed number of bits/places for exponent, you have a $E_{min}$ and a $E_{max}$ value that exponent can take. Underflow is the situation where numbers whose exponent is below the $E_{min}+1$ (remember, $E_{min}$ is already used to represent 0) exponent and hence cannot be represented (mostly as a result of an operation like subtracting two close numbers). Overflow is about the numbers whose exponent is above the $E_{max}-1$ (remember, $E_{max}$ is already used for $+-\\infty$) exponent value and hence cannot be represented as well. Note that each kind of numbers can be positive or negative depending on the sign bit/place.\n",
    "\n",
    "Subnormal numbers can help with gradual underflow. Consider for example, $x = 1.10 * 10^{E_{min}+1}$, and $y = 1.00 * 10^{E_{min}+1}$. Then $x-y = 0.1 * 10^{E_{min}+1} = 1.0 * 10^{E_{min}}$. The exponent is smaller than smallest representable value, hence result will be $0$. So, even though $x \\ne y$, $x-y = 0$. To handle these cases, we could use denormalized/subnormal numbers to represent the underflow numbers (above will be $0.1 * 10^{E_{min}}$ i.e., do not constrain that digit before the decimal point > 1). This also means we do not assume *implicit* 1, in case of binary when exponent = ${E_{min}}$. Since there could be many zeros after the decimal point, we lose precision (we have to truncate/round the small number to fit in the bits/place of `mantissa` size). Another example $a = 3.0 * 2^{-64}$ and $a * a$ is too small to be represented in 32 bit float format (IEEE-754).\n",
    "\n",
    "Also, notice that when we subtract two nearly equal quantities (and hence have matching digits for a great number of positions in `mantissa`), there will be significant loss of precision due to these large number of zeros in the fraction/mantissa.\n",
    "\n",
    "**How to represent special numbers**\n",
    "\n",
    "$\\infty$, $-\\infty$ ? We can have a convention that all 1's (in binary, or 9's in decimal) in exponent represent $\\infty$. The content of `mantissa` in this case is 0. This is because, by convention we say that max value represented is $\\beta^{E_{max}}$ and non-zero `mantissa` makes the number value greater than this (i.e., $\\infty$) which is meaningless.\n",
    "\n",
    "We still need to address another aspect: Any number, with the exception of zero, divided by zero yields respectively $\\infty$ or $-\\infty$. Dividing zero with zero results in the special NaN, the Not a Number value. NaN is useful to represent situations like addition of two infinite numbers. So, how do we represent these? Notice that when representing $\\infty$ we assumed all 0's for `mantissa`. So, we can use any non-zero as meaning it is a NaN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDPFEh-enByj"
   },
   "source": [
    "An example \n",
    "---\n",
    "\n",
    "Now background is out of our way, let's see an example of how floating points are represented in computer to get a concrete idea.\n",
    "\n",
    "We first pick how many bits we want to use to represent decimal numbers, say 8 bits.\n",
    "\n",
    "First bit (MSB) can be used for sign: 0(+ve), 1(-ve)\n",
    "Next 3 bits can be used for exponent.\n",
    "Remaining 4 bits can be used for mantissa.\n",
    "\n",
    "Let's use biased representation, so exponent value stored ranges from 0 to 7 (unsigned +ve number) with bias 3. So, actual values for exponent ranges from $E_{min} = -3$ to $E_{max} = 4$.\n",
    "\n",
    "Let's also say that exponent = 000 with mantissa = 0000 is reserved for 0.\n",
    "\n",
    "Let's say exponent = 111 mantissa = 0000 reserved for INFINITY.\n",
    "\n",
    "This will rule out -3 and 4 from being used as exponents for normal numbers. So, actual represented range is from 1 to 6 (biased representation) or as values from -2 to 3.\n",
    "\n",
    "So, we now have special cases:\n",
    "\n",
    "exponent = 000 with mantissa != 0000 => subnormal numbers\n",
    "\n",
    "exponent = 111 with mantissa != 0000 => NaN (Any number, with the exception of zero, divided by zero yields respectively ∞ or -∞. Dividing zero with zero results in the special NaN, the Not a Number value). NaN is useful to represent situations like addition of two infinites, $\\frac{0}{0}$.\n",
    "\n",
    "\n",
    "max +ve value:\n",
    "\n",
    "0 110 1111\n",
    "\n",
    "\n",
    "max -ve value:\n",
    "\n",
    "1 110 1111\n",
    "\n",
    "Smallest +ve number that can be represented:\n",
    "\n",
    "0 000 0001 (subnormal, note that mantissa cannot be 0000 as then it becomes 0).\n",
    "\n",
    "Smallest -ve number:\n",
    "\n",
    "1 000 0001 (subnormal)\n",
    "\n",
    "\n",
    "Smallest normal +ve number:\n",
    "\n",
    "0 001 0000\n",
    "\n",
    "Smallest normal -ve number:\n",
    "\n",
    "1 001 0000\n",
    "\n",
    "Gap between 0 and smallest normal +ve number = $2^{1-3} = 0.25$.\n",
    "Gap between the smallest normal +ve number and the number next = <0 001 0001> - $2^{1-3}$ = $1.0001 x 2^{1-3} - 1.0000 x 2^{1-3} = 0.0001 x 2^{1-3} = 2^{1-3-4} = 0.015625$.\n",
    "\n",
    "So, we notice that floating point representation is discrete (unlike real line), not equally spaced throughout, and finite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qs3JFwThmTeU"
   },
   "source": [
    "Measuring rounding errors\n",
    "---\n",
    "\n",
    "Note that we are representing a real number using a fixed width storage (sign+mantissa+exponent) and hence we have a rounding error. This is measured using ULP standing for units in the last place. For a real number x, when represented using a floating point number, say, fl(x), with exponent e, a least possible change in the mantissa modifies the represented value fl(x) by ${\\beta}^e * {\\beta}^{-p+1} = {\\beta}^{e-p+1}$. This is characterized as 1 ULP. So, all floating point numbers with the same exponent, $e$ have the same ULP. So, in a given representation (given radix, precision, base), 1 ULP is different for each value of the exponent and not constant throughout the representable range.\n",
    "\n",
    "How do we round? there are four types of rounding we can do. Here they are with some examples (where we assume to keep 5 digits after decimal point):\n",
    "\n",
    "* Round to nearest number representable: e.g. -0.001497 becomes -0.00150.\n",
    "* Round to zero/truncate: e.g. -0.001498 becomes -0.00149.\n",
    "* Round to +infinity (round up): e.g. -0.001498 becomes -0.00149.\n",
    "* Round to –infinity (round down): e.g. -0.001498 becomes -0.00150.\n",
    "\n",
    "\n",
    "How can we ensure the rounding is done correctly? particularly given that we cannot store digits after the lowest possible slot/digit? Use an extra digit(s) called guard digit(s). Then calculations are performed at slightly greater precision, and then stored in standard IEEE floating-point numbers after normalizing and rounding as above. Usually three extra bits are enough to ensure correctness.\n",
    "\n",
    "ULP is good for measuring rounding error. Relative error is good for measuring rounding error due to various formula (add/subtract etc).\n",
    "    \n",
    "Rounding to the nearest floating-point number corresponds to an error of less than or equal to 0.5 ULP. However, when analyzing the rounding error caused by various formulas, relative error is a better measure as it is not affected by exponent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppx6eczBaWAJ"
   },
   "source": [
    "Some proofs\n",
    "---\n",
    "\n",
    "**Rounding error introduced by nearest rounding**\n",
    "\n",
    "$\\beta = radix$ as above.\n",
    "\n",
    "let $m = \\beta - 1$\n",
    "\n",
    "let $h = \\frac{\\beta}{2}$\n",
    "\n",
    "Then, $0.h = \\frac{\\beta}{2} {\\beta}^{-1} = \\frac{1}{2}$\n",
    "\n",
    "$x = d_0.d_1 \\dots d_{p-1} d_p d_{p+1} \\dots {\\beta}^e$\n",
    "\n",
    "say $d_p \\lt h$, then we just truncate\n",
    "\n",
    "$fl(x) = d_0.d_1 \\dots d_{p-1} {\\beta}^e$\n",
    "\n",
    "\n",
    "$x - fl(x) = 0. \\dots d_{p} d_{p+1}  \\dots {\\beta}^e$\n",
    "\n",
    "$x - fl(x) = 0.d_{p} d_{p+1} \\dots {\\beta}^{-p+1} {\\beta}^e$\n",
    "\n",
    "\n",
    "$x - fl(x) \\le 0.h m m m \\dots {\\beta}^{-p+1} {\\beta}^e$\n",
    "\n",
    "$x - fl(x) \\le 0.h {\\beta}^{-p+1} {\\beta}^e$\n",
    "\n",
    "$x - fl(x) \\le \\frac{1}{2} {\\beta}^{-p+1+e}$\n",
    "\n",
    "Let $d_p \\ge h$, then we add $ \\frac{\\beta}{2} \\beta^{e-p} = \\frac{1}{2} \\beta^{e-p+1}$ and truncate.\n",
    "\n",
    "$fl(x) = d_0.d_1 \\dots d_{p-1} \\beta^e + \\frac{1}{2} \\beta^{e-p+1}$\n",
    "\n",
    "$x - fl(x) = 0.0 \\dots d_{p} d_{p+1}  \\dots {\\beta}^e -  \\frac{1}{2} \\beta^{e-p+1}$\n",
    "\n",
    "\n",
    "$x - fl(x) = 0.d_{p} d_{p+1}  \\dots {\\beta}^{e-p+1} -  \\frac{1}{2} \\beta^{e-p+1}$\n",
    "\n",
    "$x - fl(x) \\le 0.m m  \\dots {\\beta}^{e-p+1} -  \\frac{1}{2} \\beta^{e-p+1}$\n",
    "\n",
    "\n",
    "$x - fl(x) \\lt 1.0 {\\beta}^{e-p+1} -  \\frac{1}{2} \\beta^{e-p+1}$\n",
    "\n",
    "$x - fl(x) \\lt \\frac{1}{2} {\\beta}^{-p+1+e}$\n",
    "\n",
    "Also, ${\\beta}^{-p+1+e}$ = ulp(x), so rounding to nearest floating point keeps absolute error within half-ulp.\n",
    "\n",
    "\n",
    "Consider relative error,\n",
    "\n",
    "$\\frac{|x-fl(x)|}{|fl(x)|} \\lt \\frac{\\frac{1}{2} {\\beta}^{-p+1+e}}{\\beta^e} = \\frac{1}{2} \\beta^{-p+1}$\n",
    "\n",
    "(here note that  $|fl(x)| ≥ 1.0 × \\beta^e$}\n",
    "\n",
    "Relative error is indpendent of exponent, hence the same for all numbers.\n",
    "\n",
    "\n",
    "**Rounding error introduced by pure truncation is twice as much. Why?**\n",
    "\n",
    "\n",
    "$fl(x) = d_0.d_1 \\dots d_{p-1} {\\beta}^e$\n",
    "\n",
    "$x - fl(x) = 0. \\dots d_{p} d_{p+1}  \\dots {\\beta}^e$\n",
    "\n",
    "$x - fl(x) \\le 0.m m  \\dots {\\beta}^{e-p+1}$\n",
    "\n",
    "$x - fl(x) \\lt 1.0  \\dots {\\beta}^{e-p+1}$\n",
    "\n",
    "\n",
    "Operations (subtraction etc.,) using fixed bits in hardware lead to high rounding errors (the ulp between actual result vs. computed is high). Using a guard digit helps. But there is also problem of cancellation. And sometimes exact rounding (i.e,, assume infinite precision bits while operating, and round the result afterwards) is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDvPio_pmu8n"
   },
   "source": [
    "\n",
    "Arithmetic exceptions\n",
    "---\n",
    "\n",
    "Overflow condition: ±$\\infty$, f = $1111 \\dots$\n",
    "\n",
    "Underflow condition: flush to 0, ±$2^{-bias}$, [denormalized]\n",
    "\n",
    "Divide by zero: ±$\\infty$\n",
    "\n",
    "Invalid numbers: NaN\n",
    "\n",
    "Inexact value due to rounding/truncation: arithmetic operations\n",
    "\n",
    "Obviously inexact would occur very often and is usually ignored. So is the case with underflow. We may want to catch the remaining exceptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJ4XChwWdLSa"
   },
   "source": [
    "Sample code to demonstrate some of the concepts\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "swnGRZugauXf"
   },
   "outputs": [],
   "source": [
    "from decimal import *\n",
    "getcontext().prec = 28\n",
    "\n",
    "# A class for creating floating points with given number of bits for exponent and precision\n",
    "\n",
    "class MyFloat:\n",
    "  def __init__(self, exp_bits, precision):\n",
    "    self.exp_bits = exp_bits\n",
    "    self.mantissa_bits = precision - 1 # implicit 1\n",
    "    self.bias = (2**(self.exp_bits - 1)) - 1\n",
    "    self.max_exp_value = (2**self.exp_bits) - 1\n",
    "\n",
    "  def print_float(f):\n",
    "    print(bin(f))\n",
    "\n",
    "  def create(self, s, e, m):\n",
    "    return (s << (self.exp_bits + self.mantissa_bits) | (e << self.mantissa_bits) | m)\n",
    "\n",
    "  def sign(self, f):\n",
    "    return f >> (self.exp_bits + self.mantissa_bits)\n",
    "\n",
    "  def exp(self, f):\n",
    "    return ((f >> self.mantissa_bits) & (2**self.exp_bits -1))\n",
    "\n",
    "  def mantissa(self, f):\n",
    "    return f & ((2**self.mantissa_bits) - 1)\n",
    "\n",
    "  def fraction(self, f):\n",
    "\n",
    "    fracs = []\n",
    "    for i in range(0, self.mantissa_bits):\n",
    "      e = 2**(self.mantissa_bits-i)\n",
    "      fracs.append( (f & 1)/e )\n",
    "      f = f >> 1\n",
    "    \n",
    "    return sum(fracs)\n",
    "\n",
    "  def _epsilon(self):\n",
    "    return -self.mantissa_bits\n",
    "\n",
    "  def ulp(self, n):\n",
    "\n",
    "    e = self.exp(n)\n",
    "    s = self.sign(n)\n",
    "    m = self.mantissa(n)\n",
    "\n",
    "    r = self._special(e, s, m)\n",
    "\n",
    "    if r is not None:\n",
    "      return r\n",
    " \n",
    "    e = e - self.bias\n",
    "    return \"2**\" + str(self._epsilon() + e)\n",
    "\n",
    "  def as_binary(self, f):\n",
    "\n",
    "    # TODO: handle subnormal and special numbers\n",
    "\n",
    "    s = 0\n",
    "\n",
    "    if f < 0:\n",
    "      s = 1\n",
    "      f = -f\n",
    "    \n",
    "    e = 0\n",
    "    while f < 1:\n",
    "      f *= 2\n",
    "      e -= 1\n",
    "    \n",
    "    while f > 2:\n",
    "      f = f/2\n",
    "      e += 1\n",
    "\n",
    "    e = e + self.bias\n",
    "\n",
    "    f -= 1 # implicit 1\n",
    "\n",
    "    mbits = []\n",
    "\n",
    "    \n",
    "    if (f != 0):\n",
    "      while True:\n",
    "        \n",
    "        f *= 2\n",
    "        if f > 1:\n",
    "          f -= 1\n",
    "          mbits.append(\"1\")\n",
    "        elif f < 1:\n",
    "          mbits.append(\"0\")\n",
    "        else:\n",
    "          mbits.append(\"1\")\n",
    "          break\n",
    "        \n",
    "        if (len(mbits) == self.mantissa_bits):\n",
    "          break\n",
    "\n",
    "    txt = \"{0} \" + \"{0:b}\".format(e).zfill(self.exp_bits) + \" \" + \"\".join(mbits).zfill(self.mantissa_bits)\n",
    "    return txt.format(s)\n",
    "\n",
    "  def _special(self, e, s, m):\n",
    "    if (e == self.max_exp_value) and (m == 0) and (s == 0):\n",
    "      return \"+Infinity\"\n",
    "\n",
    "    if (e == self.max_exp_value) and (m == 0) and (s == 1):\n",
    "      return \"-Infinity\"\n",
    "\n",
    "    if (e == self.max_exp_value and s == 0):\n",
    "      return \"NaN\"\n",
    "    if (e == self.max_exp_value and s == 1):\n",
    "      return \"-NaN\"\n",
    "    \n",
    "    return None\n",
    "\n",
    "  def as_decimal(self, n):\n",
    "    e = self.exp(n)\n",
    "    s = self.sign(n)\n",
    "    m = self.mantissa(n)\n",
    "\n",
    "    r = self._special(e, s, m)\n",
    "\n",
    "    if r is not None:\n",
    "      return r\n",
    "\n",
    "    frac = self.fraction(m)\n",
    "    if e > 0 and e < self.max_exp_value:\n",
    "      # normalized numbers, add implicit 1\n",
    "      frac += 1.0\n",
    "\n",
    "    e -= self.bias\n",
    "\n",
    "    if e == -self.bias:\n",
    "      e += 1 # subnormal numbers, representable numbers which are immediately close to smallest normal number \n",
    "      # additionally, if we keep e=-3, then numbers smaller than smallest are represented causing confusion\n",
    "      # in this case we also cannot use assumed b0 = 1 as that leads to duplicate numbers, for example then one with when e=-2 already\n",
    "\n",
    "    return  Decimal((-1)**self.sign(n)) * Decimal(2**e) * Decimal(frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p4LEm1dhg43-",
    "outputId": "83543bbd-078c-4774-e736-c7358fc307a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0b0\n",
      "0b10001001\n",
      "0b1100000000000000\n",
      "23\n",
      "1030.000000000\n",
      "2**-13\n",
      "0.25 => 2**-25\n",
      "2    => 2**-22\n",
      "3    => 2**-22\n",
      "4    => 2**-21\n",
      "10   => 2**-20\n",
      "100  => 2**-17\n",
      "1030 => 2**-13\n",
      "1030 => 2**-13\n",
      "Bits for 0.25 = 0 01111101 00000000000000000000000\n",
      "Bits for  0.1 = 0 01111011 10011001100110011001100\n"
     ]
    }
   ],
   "source": [
    "# Create IEEE 754 style single float and print details to test the MyFloat class above\n",
    "\n",
    "myFloat32 = MyFloat(8, 24)\n",
    "\n",
    "def print_details(n):\n",
    "  print(bin(myFloat32.sign(n)))\n",
    "  print(bin(myFloat32.exp(n)))\n",
    "  print(bin(myFloat32.mantissa(n)))\n",
    "  print(myFloat32.mantissa_bits)\n",
    "  print(myFloat32.as_decimal(n))\n",
    "  print(myFloat32.ulp(n))\n",
    "\n",
    "#print_details(0x3E800000)\n",
    "#print_details(0x3E1BA5E3) # 1.5199999511241912841796875E-1\n",
    "\n",
    "#print_details(0x3F7F7CEE)  # 9.9800002574920654296875E-1\n",
    "\n",
    "print_details(0x4480C000)   # 1030.000000000\n",
    "\n",
    "print(\"0.25 => \" + myFloat32.ulp(0x3E800000))\n",
    "print(\"2    => \" + myFloat32.ulp(0x40000000))\n",
    "print(\"3    => \" + myFloat32.ulp(0x40400000))\n",
    "print(\"4    => \" + myFloat32.ulp(0x40800000))\n",
    "print(\"10   => \" + myFloat32.ulp(0x41200000))\n",
    "print(\"100  => \" + myFloat32.ulp(0x42C80000))\n",
    "print(\"1030 => \" + myFloat32.ulp(0x4480C000))\n",
    "\n",
    "print(\"1030 => \" + myFloat32.ulp(0x4480C000))\n",
    "\n",
    "#verified above are correct Ulp by testing through https://docs.oracle.com/javase/1.5.0/docs/api/java/lang/Math.html#ulp(float)\n",
    "\n",
    "print(\"Bits for 0.25 =\", myFloat32.as_binary(0.25))\n",
    "\n",
    "print(\"Bits for  0.1 =\", myFloat32.as_binary(0.1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUi6Epo2qBr_"
   },
   "source": [
    "Sample floating point number system\n",
    "---\n",
    "\n",
    "Create a floating point number type with 1 bit for sign, 8 bits for exponent, and 5 bits (1 implicit and 4 explicit) for mantissa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fw_5QHoFR-Ce",
    "outputId": "99726a87-fdef-4238-e404-36d3c4131aae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon-4\n",
      "N = 0 -> 0.00\n",
      "N = 1 -> 0.015625\n",
      "N = 2 -> 0.03125\n",
      "N = 3 -> 0.046875\n",
      "N = 4 -> 0.0625\n",
      "N = 5 -> 0.078125\n",
      "N = 6 -> 0.09375\n",
      "N = 7 -> 0.109375\n",
      "N = 8 -> 0.125\n",
      "N = 9 -> 0.140625\n",
      "N = 10 -> 0.15625\n",
      "N = 11 -> 0.171875\n",
      "N = 12 -> 0.1875\n",
      "N = 13 -> 0.203125\n",
      "N = 14 -> 0.21875\n",
      "N = 15 -> 0.234375\n",
      "N = 16 -> 0.25\n",
      "N = 17 -> 0.265625\n",
      "N = 18 -> 0.28125\n",
      "N = 19 -> 0.296875\n",
      "N = 20 -> 0.3125\n",
      "N = 21 -> 0.328125\n",
      "N = 22 -> 0.34375\n",
      "N = 23 -> 0.359375\n",
      "N = 24 -> 0.375\n",
      "N = 25 -> 0.390625\n",
      "N = 26 -> 0.40625\n",
      "N = 27 -> 0.421875\n",
      "N = 28 -> 0.4375\n",
      "N = 29 -> 0.453125\n",
      "N = 30 -> 0.46875\n",
      "N = 31 -> 0.484375\n",
      "N = 32 -> 0.5\n",
      "N = 33 -> 0.53125\n",
      "N = 34 -> 0.5625\n",
      "N = 35 -> 0.59375\n",
      "N = 36 -> 0.625\n",
      "N = 37 -> 0.65625\n",
      "N = 38 -> 0.6875\n",
      "N = 39 -> 0.71875\n",
      "N = 40 -> 0.75\n",
      "N = 41 -> 0.78125\n",
      "N = 42 -> 0.8125\n",
      "N = 43 -> 0.84375\n",
      "N = 44 -> 0.875\n",
      "N = 45 -> 0.90625\n",
      "N = 46 -> 0.9375\n",
      "N = 47 -> 0.96875\n",
      "N = 48 -> 1\n",
      "N = 49 -> 1.0625\n",
      "N = 50 -> 1.125\n",
      "N = 51 -> 1.1875\n",
      "N = 52 -> 1.25\n",
      "N = 53 -> 1.3125\n",
      "N = 54 -> 1.375\n",
      "N = 55 -> 1.4375\n",
      "N = 56 -> 1.5\n",
      "N = 57 -> 1.5625\n",
      "N = 58 -> 1.625\n",
      "N = 59 -> 1.6875\n",
      "N = 60 -> 1.75\n",
      "N = 61 -> 1.8125\n",
      "N = 62 -> 1.875\n",
      "N = 63 -> 1.9375\n",
      "N = 64 -> 2\n",
      "N = 65 -> 2.1250\n",
      "N = 66 -> 2.250\n",
      "N = 67 -> 2.3750\n",
      "N = 68 -> 2.50\n",
      "N = 69 -> 2.6250\n",
      "N = 70 -> 2.750\n",
      "N = 71 -> 2.8750\n",
      "N = 72 -> 3.0\n",
      "N = 73 -> 3.1250\n",
      "N = 74 -> 3.250\n",
      "N = 75 -> 3.3750\n",
      "N = 76 -> 3.50\n",
      "N = 77 -> 3.6250\n",
      "N = 78 -> 3.750\n",
      "N = 79 -> 3.8750\n",
      "N = 80 -> 4\n",
      "N = 81 -> 4.2500\n",
      "N = 82 -> 4.500\n",
      "N = 83 -> 4.7500\n",
      "N = 84 -> 5.00\n",
      "N = 85 -> 5.2500\n",
      "N = 86 -> 5.500\n",
      "N = 87 -> 5.7500\n",
      "N = 88 -> 6.0\n",
      "N = 89 -> 6.2500\n",
      "N = 90 -> 6.500\n",
      "N = 91 -> 6.7500\n",
      "N = 92 -> 7.00\n",
      "N = 93 -> 7.2500\n",
      "N = 94 -> 7.500\n",
      "N = 95 -> 7.7500\n",
      "N = 96 -> 8\n",
      "N = 97 -> 8.5000\n",
      "N = 98 -> 9.000\n",
      "N = 99 -> 9.5000\n",
      "N = 100 -> 10.00\n",
      "N = 101 -> 10.5000\n",
      "N = 102 -> 11.000\n",
      "N = 103 -> 11.5000\n",
      "N = 104 -> 12.0\n",
      "N = 105 -> 12.5000\n",
      "N = 106 -> 13.000\n",
      "N = 107 -> 13.5000\n",
      "N = 108 -> 14.00\n",
      "N = 109 -> 14.5000\n",
      "N = 110 -> 15.000\n",
      "N = 111 -> 15.5000\n",
      "N = 112 -> +Infinity\n",
      "N = 113 -> NaN\n",
      "N = 114 -> NaN\n",
      "N = 115 -> NaN\n",
      "N = 116 -> NaN\n",
      "N = 117 -> NaN\n",
      "N = 118 -> NaN\n",
      "N = 119 -> NaN\n",
      "N = 120 -> NaN\n",
      "N = 121 -> NaN\n",
      "N = 122 -> NaN\n",
      "N = 123 -> NaN\n",
      "N = 124 -> NaN\n",
      "N = 125 -> NaN\n",
      "N = 126 -> NaN\n",
      "N = 127 -> NaN\n",
      "N = 128 -> -0.00\n",
      "N = 129 -> -0.015625\n",
      "N = 130 -> -0.03125\n",
      "N = 131 -> -0.046875\n",
      "N = 132 -> -0.0625\n",
      "N = 133 -> -0.078125\n",
      "N = 134 -> -0.09375\n",
      "N = 135 -> -0.109375\n",
      "N = 136 -> -0.125\n",
      "N = 137 -> -0.140625\n",
      "N = 138 -> -0.15625\n",
      "N = 139 -> -0.171875\n",
      "N = 140 -> -0.1875\n",
      "N = 141 -> -0.203125\n",
      "N = 142 -> -0.21875\n",
      "N = 143 -> -0.234375\n",
      "N = 144 -> -0.25\n",
      "N = 145 -> -0.265625\n",
      "N = 146 -> -0.28125\n",
      "N = 147 -> -0.296875\n",
      "N = 148 -> -0.3125\n",
      "N = 149 -> -0.328125\n",
      "N = 150 -> -0.34375\n",
      "N = 151 -> -0.359375\n",
      "N = 152 -> -0.375\n",
      "N = 153 -> -0.390625\n",
      "N = 154 -> -0.40625\n",
      "N = 155 -> -0.421875\n",
      "N = 156 -> -0.4375\n",
      "N = 157 -> -0.453125\n",
      "N = 158 -> -0.46875\n",
      "N = 159 -> -0.484375\n",
      "N = 160 -> -0.5\n",
      "N = 161 -> -0.53125\n",
      "N = 162 -> -0.5625\n",
      "N = 163 -> -0.59375\n",
      "N = 164 -> -0.625\n",
      "N = 165 -> -0.65625\n",
      "N = 166 -> -0.6875\n",
      "N = 167 -> -0.71875\n",
      "N = 168 -> -0.75\n",
      "N = 169 -> -0.78125\n",
      "N = 170 -> -0.8125\n",
      "N = 171 -> -0.84375\n",
      "N = 172 -> -0.875\n",
      "N = 173 -> -0.90625\n",
      "N = 174 -> -0.9375\n",
      "N = 175 -> -0.96875\n",
      "N = 176 -> -1\n",
      "N = 177 -> -1.0625\n",
      "N = 178 -> -1.125\n",
      "N = 179 -> -1.1875\n",
      "N = 180 -> -1.25\n",
      "N = 181 -> -1.3125\n",
      "N = 182 -> -1.375\n",
      "N = 183 -> -1.4375\n",
      "N = 184 -> -1.5\n",
      "N = 185 -> -1.5625\n",
      "N = 186 -> -1.625\n",
      "N = 187 -> -1.6875\n",
      "N = 188 -> -1.75\n",
      "N = 189 -> -1.8125\n",
      "N = 190 -> -1.875\n",
      "N = 191 -> -1.9375\n",
      "N = 192 -> -2\n",
      "N = 193 -> -2.1250\n",
      "N = 194 -> -2.250\n",
      "N = 195 -> -2.3750\n",
      "N = 196 -> -2.50\n",
      "N = 197 -> -2.6250\n",
      "N = 198 -> -2.750\n",
      "N = 199 -> -2.8750\n",
      "N = 200 -> -3.0\n",
      "N = 201 -> -3.1250\n",
      "N = 202 -> -3.250\n",
      "N = 203 -> -3.3750\n",
      "N = 204 -> -3.50\n",
      "N = 205 -> -3.6250\n",
      "N = 206 -> -3.750\n",
      "N = 207 -> -3.8750\n",
      "N = 208 -> -4\n",
      "N = 209 -> -4.2500\n",
      "N = 210 -> -4.500\n",
      "N = 211 -> -4.7500\n",
      "N = 212 -> -5.00\n",
      "N = 213 -> -5.2500\n",
      "N = 214 -> -5.500\n",
      "N = 215 -> -5.7500\n",
      "N = 216 -> -6.0\n",
      "N = 217 -> -6.2500\n",
      "N = 218 -> -6.500\n",
      "N = 219 -> -6.7500\n",
      "N = 220 -> -7.00\n",
      "N = 221 -> -7.2500\n",
      "N = 222 -> -7.500\n",
      "N = 223 -> -7.7500\n",
      "N = 224 -> -8\n",
      "N = 225 -> -8.5000\n",
      "N = 226 -> -9.000\n",
      "N = 227 -> -9.5000\n",
      "N = 228 -> -10.00\n",
      "N = 229 -> -10.5000\n",
      "N = 230 -> -11.000\n",
      "N = 231 -> -11.5000\n",
      "N = 232 -> -12.0\n",
      "N = 233 -> -12.5000\n",
      "N = 234 -> -13.000\n",
      "N = 235 -> -13.5000\n",
      "N = 236 -> -14.00\n",
      "N = 237 -> -14.5000\n",
      "N = 238 -> -15.000\n",
      "N = 239 -> -15.5000\n",
      "N = 240 -> -Infinity\n",
      "N = 241 -> -NaN\n",
      "N = 242 -> -NaN\n",
      "N = 243 -> -NaN\n",
      "N = 244 -> -NaN\n",
      "N = 245 -> -NaN\n",
      "N = 246 -> -NaN\n",
      "N = 247 -> -NaN\n",
      "N = 248 -> -NaN\n",
      "N = 249 -> -NaN\n",
      "N = 250 -> -NaN\n",
      "N = 251 -> -NaN\n",
      "N = 252 -> -NaN\n",
      "N = 253 -> -NaN\n",
      "N = 254 -> -NaN\n",
      "N = 255 -> -NaN\n"
     ]
    }
   ],
   "source": [
    "# Create a floating point number type with 1 bit for sign, 8 bits for exponent, and 5 bits (1 implicit and 4 explicit) for mantissa\n",
    "\n",
    "myFloat = MyFloat(3, 5)\n",
    "\n",
    "print(\"Epsilon\" + str(myFloat._epsilon()))\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "\n",
    "for i in range(0, 256):\n",
    "  xs.append(i)\n",
    "  print(\"N =\", i, \"-> \", end='')\n",
    "  y = myFloat.as_decimal(i)\n",
    "  print(y)\n",
    "  if (i > 111 and i < 128) or (i > 239 and i < 256):\n",
    "    ys.append(0)\n",
    "  else:\n",
    "    ys.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "id": "RyWrslXOKo2S",
    "outputId": "cf2cba07-d6b9-4906-e5a7-875708e7d37f"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1920x240 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Density plot of the representable numbers for above example\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(16, 2), dpi=120)\n",
    "\n",
    "values = ys[0:112] \n",
    "\n",
    "plt.scatter(values, np.zeros_like(values), cmap=\"hot_r\", vmin=-2)\n",
    "plt.xticks(np.linspace(0, 16, 20))\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "RsQSd-W9Uoki",
    "outputId": "6181b4dd-a6f9-4cb0-8dc7-8cdb5a37a50e"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# density histogram of floating point numbers for above example\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lys = []\n",
    "\n",
    "for i in range(16, 112):\n",
    "  ly = myFloat.as_decimal(i)\n",
    "  lys.append(ly)\n",
    "\n",
    "plt.hist(lys, 'auto', facecolor='g', alpha=0.75)\n",
    "plt.xlabel('floating point range')\n",
    "plt.ylabel('count')\n",
    "plt.title('Histogram showing floating point packing')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "kqPaTAzctdrZ",
    "outputId": "401b7e23-f57a-4e92-c97f-fd99deacedbb"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# density histogram of log of floating point numbers for above example\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist([np.log(float(x)) for x in lys], 'auto', facecolor='g', alpha=0.75)\n",
    "\n",
    "plt.xlabel('log-floating point range')\n",
    "plt.ylabel('count')\n",
    "plt.title('Histogram showing log-linear packing of floating point numbers')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Os9XjfKW5OnO"
   },
   "source": [
    "Exercises\n",
    "---\n",
    "\n",
    "* Understand half precision float vs bfloat16 representation\n",
    "* Demonstrate how the knowledge of floating point representation is useful:\n",
    "  * Convert from float to bfloat16.\n",
    "    * Execute an example arithmetic expression using both half-float and bfloat16 and see the difference in results and explain.\n",
    "  * Fast square root by halving exponent directly\n",
    "* Machine epsilon: when two successive iterates differ by less than $|\\epsilon|$, we may assume that the iteration has converged and stop the process. Provide a demonstration of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hny6vhED4sL0"
   },
   "source": [
    "References\n",
    "---\n",
    "\n",
    "* https://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html\n",
    "* https://www.exploringbinary.com/the-spacing-of-binary-floating-point-numbers/\n",
    "* https://softwareengineering.stackexchange.com/questions/215065/can-anyone-explain-representation-of-float-in-memory\n",
    "* https://stackoverflow.com/questions/55253233/convert-fp32-to-bfloat16-in-c/55254307#55254307\n",
    "* https://software.intel.com/content/www/us/en/develop/articles/intel-deep-learning-boost-new-instruction-bfloat16.html\n",
    "* http://www.binaryconvert.com/result_float.html?decimal=048046049053054050053\n",
    "* http://www.cs.utep.edu/interval-comp/hayes.pdf\n",
    "* https://stackoverflow.com/questions/40082459/what-is-overflow-and-underflow-in-floating-point\n",
    "* Floating point vs. bigint: https://stackoverflow.com/a/6320218\n",
    "* http://www.cs.jhu.edu/~jorgev/cs333/readings/8-Bit_Floating_Point.pdf\n",
    "* https://babbage.cs.qc.cuny.edu/IEEE-754.old/Decimal.html\n",
    "* https://www.youtube.com/watch?v=p8u_k2LIZyo\n",
    "* https://www-ljk.imag.fr/membres/Carine.Lucas/TPScilab/JMMuller/ulp-toms.pdf\n",
    "* https://matthew-brett.github.io/teaching/floating_error.html\n",
    "* http://www.math.pitt.edu/~trenchea/math1070/MATH1070_2_Error_and_Computer_Arithmetic.pdf\n",
    "* http://home.iitk.ac.in/~pranab/ESO208/rajesh/03-04/Errors.pdf\n",
    "* https://stackoverflow.com/a/7524916\n",
    "* http://www.cas.mcmaster.ca/~qiao/courses/cas708/slides/ch01.pdf\n",
    "* https://stackoverflow.com/questions/43965347/ulp-unit-of-least-precision\n",
    "* Algorithms for standard operations: https://www.rfwireless-world.com/Tutorials/floating-point-tutorial.html\n",
    "* https://www.sciencedirect.com/topics/computer-science/fixed-point-number\n",
    "* https://stackoverflow.com/questions/51170944/understanding-the-usefulness-of-denormalized-floating-point-numbers\n",
    "* https://www.ias.ac.in/public/Volumes/reso/021/01/0011-0030.pdf\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FloatingPoints.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
